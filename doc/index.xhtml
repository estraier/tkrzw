<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ja" xml:lang="ja">
<head>
<meta charset="UTF-8"/>
<title>Tkrzw: a set of implementations of DBM</title>
<link href="prism.css" rel="stylesheet"/>
<link href="tk-icon.png" rel="icon" type="image/png" sizes="144x144"/>
<style>/*<![CDATA[*/
html,body,article,p,pre,code,li,dt,dd,td,th,div { font-size: 12pt; }
html { margin: 0; padding: 0; background: #eeeeee; }
body { width: 100%; margin: 0; padding: 0; background: #eeeeee; text-align: center; }
body { animation: fadeIn 0.8s ease 0s 1 normal; -webkit-animation: fadeIn 0.8s ease 0s 1 normal; }
article { display: inline-block; max-width: 100ex; overflow: hidden; border: 1px solid #aaaaaa; border-radius: 2ex;
  margin: 2ex 1ex; padding: 3ex 3ex; background: #ffffff; text-align: left; line-height: 1.6; color: #111111; }
h1,h2,h3,h4,h5,h6 { color: #000000; margin: 2ex 0 0 0; text-indent: 0; }
h1 { text-align: center; margin: 2ex 0 3ex 0; }
p { text-indent: 2ex; }
pre { margin-left: 1.5ex; padding: 0.4ex 0.6ex; border: solid 1px #dddddd; border-radius: 0.5ex;
  white-space: pre-wrap; word-wrap: break-word; line-height: 1.2; text-indent: 0; font-size: 10pt; }
code { font-weight: bold; }
pre[class*="language-"] { font-size: 10pt; line-height: 120%; padding: 0.5ex 0.8ex; background: #f8f8f8;
  max-height: 70ex; }
li { margin-left: 0.2ex; }
dt { margin-left: 2.0ex; }
dd { margin-left: 5.0ex; }
table { margin-left: 1.5ex; border-collapse: collapse; }
td,th { padding: 0 0.5ex; border: 1px solid #dddddd; }
td { font-size: 11pt; }
td.num { text-align: right; }
th { font-size: 10pt; font-weight: normal; background: #eeeeee; }
a { color: #004488; }
div.logo { text-align: center; }
div.logo img { max-width: 30ex; }
div.dbstructure { text-align: center; }
div.dbstructure img { max-width: 70ex; }
h2 a.headanc, h3 a.headanc {
  display: none;
  font-size: 8pt;
  vertical-align: super;
  padding-left: 0.5ex;
}
h2:hover a.headanc, h3:hover a.headanc {
  display: inline;
  font-size: 10pt;
  vertical-align: super;
}
/*]]>*/</style>
<script type="text/javascript">/*<![CDATA[*/
window.onload = function(){
  for (let tag of ['h2', 'h3']) {
    for (let header of document.getElementsByTagName(tag)) {
      let anchor = document.createElement('a');
      anchor.textContent = "#"
      anchor.href = "#" + header.id;
      anchor.className = "headanc";
      header.appendChild(anchor);
    }
  }
  let elem = document.getElementById("title");
  let now = new Date();
  if ((now.getFullYear() + now.getMonth() + now.getDate() + now.getHours()) % 5 == 0) {
    let texts = [
      "Tokyo and Kyoto Raging Zigzag Wars",
      "Table Knife Ripping Zodiac Wheel",
      "Traveling Kangaroos Riding Zeebras Wildly",
      "Thirsty Kids Running Zoos in Waikiki",
      "Timid Killers Rubbing Zombie Wax",
      "Teramachi Kyogoku Rokkaku Zezeura Wakamiya",
      "Tonikaku Kawaii Randoseru Zettai Wasurenai",
      "Tibetan Kind Rangers Zipping Waffles",
      "Tetsuya Komuro Raving Zone Walker",
    ];
    let text = texts[Math.floor(now.getTime() / 60000) % texts.length];
    elem.firstChild.nodeValue = "Tkrzw: " + text;
  }
}
/*]]>*/</script>
</head>
<body>
<script src="prism.js"/>
<article>

<h1 id="title">Tkrzw: a set of implementations of DBM</h1>

<div class="logo"><img src="ilove-logo.png"/></div>

<h2 id="overview">Overview</h2>

<p>DBM (Database Manager) is a concept of libraries to store an associative array on a permanent storage.  In other words, DBM allows an application program to store key-value pairs in a file and reuse them later.  Each of keys and values is a string or a sequence of bytes.  The key of each record must be unique within the database and a value is associated to it.  You can retrieve a stored record with its key very quickly.  Thanks to simple structure of DBM, its performance can be extremely high.</p>

<p>Tkrzw is a C++ library implementing DBM with various algorithms.  It features high degrees of performance, concurrency, scalability and durability.  The following classes are provided.</p>

<ul>
<li><b><a href="#hashdbm_overview">HashDBM</a></b> : File datatabase manager implementation based on hash table.</li>
<li><b><a href="#treedbm_overview">TreeDBM</a></b> : File datatabase manager implementation based on B+ tree.</li>
<li><b><a href="#skipdbm_overview">SkipDBM</a></b> : File datatabase manager implementation based on skip list.</li>
<li><b><a href="#tinydbm_overview">TinyDBM</a></b> : On-memory datatabase manager implementation based on hash table.</li>
<li><b><a href="#babydbm_overview">BabyDBM</a></b> : On-memory datatabase manager implementation based on B+ tree.</li>
<li><b><a href="#cachedbm_overview">CacheDBM</a></b> : On-memory datatabase manager implementation with LRU deletion.</li>
<li><b><a href="#stddbm_overview">Std(Hash|Tree)DBM</a></b> : On-memory DBM implementations using std::unordered_map and std::map.</li>
<li><b><a href="#polydbm_overview">(Poly|Shard)DBM</a></b> : Polymorphic and sharding datataba manager adapters.</li>
<li><b><a href="#index_overview">(File|Mem)Index</a></b> : Secondary index implementations.</li>
</ul>

<p>All database classes share the same interface so that applications can use any of them with the common API.  All classes are thread-safe so that multiple threads can access the same database simultaneously.  Basically, you can store records with the "Set" method, retrieve records with the "Get" method, and remove records with the "Remove" method.  Iterator is also supported to retrieve each and every record of the database.</p>

<p>Each database class has specific traits.  It is important to select the best one or combine some of them for the use case.</p>

<table>
<tr>
<th>class</th>
<th>medium</th>
<th>algorithm</th>
<th>ordered<br/>access</th>
<th>time<br/>complexity</th>
<th>update<br/>method</th>
<th>threading<br/>mutex</th>
<th>typical<br/>footprint</th>
</tr>
<tr>
<td>HashDBM</td>
<td>File</td>
<td>Hash table</td>
<td>No</td>
<td>O(1)</td>
<td>Online</td>
<td>Record RW-lock</td>
<td>14 bytes/rec</td>
</tr>
<tr>
<td>TreeDBM</td>
<td>File</td>
<td>B+ tree</td>
<td>Yes</td>
<td>O(log N)</td>
<td>Online</td>
<td>Page RW-lock</td>
<td>3 bytes/rec</td>
</tr>
<tr>
<td>SkipDBM</td>
<td>File</td>
<td>Skip list</td>
<td>Yes</td>
<td>O(log N)</td>
<td>Batch</td>
<td>Whole RW-lock</td>
<td>4 bytes/rec</td>
</tr>
<tr>
<td>TinyDBM</td>
<td>RAM</td>
<td>Hash table</td>
<td>No</td>
<td>O(1)</td>
<td>Online</td>
<td>Record RW-lock</td>
<td>-</td>
</tr>
<tr>
<td>BabyDBM</td>
<td>RAM</td>
<td>B+ tree</td>
<td>Yes</td>
<td>O(log N)</td>
<td>Online</td>
<td>Page RW-lock</td>
<td>-</td>
</tr>
<tr>
<td>CacheDBM</td>
<td>RAM</td>
<td>Hash table</td>
<td>No</td>
<td>O(1)</td>
<td>Online</td>
<td>Slot RW-lock</td>
<td>-</td>
</tr>
<tr>
<td>StdHashDBM</td>
<td>RAM</td>
<td>Hash table</td>
<td>No</td>
<td>O(1)</td>
<td>Online</td>
<td>Whole RW-lock</td>
<td>-</td>
</tr>
<tr>
<td>StdTreeDBM</td>
<td>RAM</td>
<td>Red-black tree</td>
<td>Yes</td>
<td>O(log N)</td>
<td>Online</td>
<td>Whole RW-lock</td>
<td>-</td>
</tr>
</table>

<p>Performance is also different among the database classes.  Let's say, one thread sets 1 million records.  The key of each record is an 8-byte string from "00000000", "00000001", to "00999999" in ascending order.  The value is an 8-byte string.  Then, it retrieves all records and finally remove them.  The following table shows throughput of all operations on a computer with a 3.5Ghz 6-core CPU.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">1,426,925 QPS</td>
<td class="num">1,734,473 QPS</td>
<td class="num">1,207,978 QPS</td>
<td class="num">28.4 MB</td>
<td class="num">26.8 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">1,506,575 QPS</td>
<td class="num">1,612,324 QPS</td>
<td class="num">1,418,317 QPS</td>
<td class="num">67.9 MB</td>
<td class="num">17.8 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">1,144,860 QPS</td>
<td class="num">394,145 QPS</td>
<td class="num">1,211,729 QPS</td>
<td class="num">64.8 MB</td>
<td class="num">19.3 MB</td>
</tr>
<tr>
<td>TinyDBM</td>
<td class="num">2,506,691 QPS</td>
<td class="num">2,929,785 QPS</td>
<td class="num">2,744,441 QPS</td>
<td class="num">54.7 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>BabyDBM</td>
<td class="num">2,080,122 QPS</td>
<td class="num">2,331,687 QPS</td>
<td class="num">2,084,949 QPS</td>
<td class="num">61.1 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>CacheDBM</td>
<td class="num">2,505,286 QPS</td>
<td class="num">3,115,558 QPS</td>
<td class="num">2,858,990 QPS</td>
<td class="num">73.7 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdHashDBM</td>
<td class="num">2,012,894 QPS</td>
<td class="num">2,920,263 QPS</td>
<td class="num">2,661,896 QPS</td>
<td class="num">99.1 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdTreeDBM</td>
<td class="num">1,331,320 QPS</td>
<td class="num">2,957,670 QPS</td>
<td class="num">2,751,585 QPS</td>
<td class="num">104.5 MB</td>
<td class="num">n/a</td>
</tr>
</table>

<p>Next, we use 10 threads, each of which sets 100 thousand records.  Thus, 1 million records in total are set.  Then, the threads retrieve all records and finally remove them.  As seen below, the on-memory hash database is extremely good at concurrent updating.  The the file hash database is also very good at concurrent updating although it is based on the file storage.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">2,064,218 QPS</td>
<td class="num">4,011,795 QPS</td>
<td class="num">2,048,902 QPS</td>
<td class="num">28.7 MB</td>
<td class="num">26.8 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">854,502 QPS</td>
<td class="num">2,525,617 QPS</td>
<td class="num">700,064 QPS</td>
<td class="num">63.8 MB</td>
<td class="num">19.0 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">762,903 QPS</td>
<td class="num">1,520,528 QPS</td>
<td class="num">886,466 QPS</td>
<td class="num">80.8 MB</td>
<td class="num">19.3 MB</td>
</tr>
<tr>
<td>TinyDBM</td>
<td class="num">5,058,370 QPS</td>
<td class="num">8,370,093 QPS</td>
<td class="num">6,611,346 QPS</td>
<td class="num">54.8 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>BabyDBM</td>
<td class="num">1,136,897 QPS</td>
<td class="num">7,395,522 QPS</td>
<td class="num">1,213,289 QPS</td>
<td class="num">50.5 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>CacheDBM</td>
<td class="num">5,238,041 QPS</td>
<td class="num">7,387,499 QPS</td>
<td class="num">7,382,310 QPS</td>
<td class="num">74.0 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdHashDBM</td>
<td class="num">804,870 QPS</td>
<td class="num">8,667,923 QPS</td>
<td class="num">1,131,562 QPS</td>
<td class="num">99.3 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdTreeDBM</td>
<td class="num">387,786 QPS</td>
<td class="num">8,069,927 QPS</td>
<td class="num">1,128,146 QPS</td>
<td class="num">106.6 MB</td>
<td class="num">n/a</td>
</tr>
</table>

<p>The above test cases should show ideal performance of each database class because records are accessed sequentially and the entire data can be cached on memory by the file system.  Then, let's move on to tougher test cases.  We build a large database of 100 million records with random keys between "00000000" and "99999999".  As some keys are duplicated and such records are overwritten, the actual number of unique records is about 63 million.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">1,269,066 QPS</td>
<td class="num">1,528,441 QPS</td>
<td class="num">1,302,638 QPS</td>
<td class="num">1787.6 MB</td>
<td class="num">1828.2 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">185,871 QPS</td>
<td class="num">129,585 QPS</td>
<td class="num">322,534 QPS</td>
<td class="num">3903.3 MB</td>
<td class="num">1596.8 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">432,946 QPS</td>
<td class="num">204,870 QPS</td>
<td class="num">843,037 QPS</td>
<td class="num">3036.5 MB</td>
<td class="num">1225.7 MB</td>
</tr>
<tr>
<td>TinyDBM</td>
<td class="num">2,267,644 QPS</td>
<td class="num">2,576,552 QPS</td>
<td class="num">2,690,487 QPS</td>
<td class="num">3573.0 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>BabyDBM</td>
<td class="num">490,861 QPS</td>
<td class="num">493,863 QPS</td>
<td class="num">489,624 QPS</td>
<td class="num">2963.1 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>CacheDBM</td>
<td class="num">2,263,022 QPS</td>
<td class="num">2,257,323 QPS</td>
<td class="num">2,745,666 QPS</td>
<td class="num">4999.2 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdHashDBM</td>
<td class="num">1,691,914 QPS</td>
<td class="num">2,131,297 QPS</td>
<td class="num">2,315,516 QPS</td>
<td class="num">6409.0 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdTreeDBM</td>
<td class="num">463,928 QPS</td>
<td class="num">516,673 QPS</td>
<td class="num">505,744 QPS</td>
<td class="num">6593.6 MB</td>
<td class="num">n/a</td>
</tr>
</table>

<p>Finally, we do the same operations with 10 threads.  These test cases aim to simulate typical use cases where a large amount of random records are inserted and retrieved simultaneously.  As suggested by the time complexity O(1) in theory, actual throughputs of the file hash database and the on-memory hash database are not affected by access patterns or the number of records.  Meanwhile, throughputs of the file tree database and the file skip database decline although they can still respond to hundreds of thousands of queries per second.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">2,134,418 QPS</td>
<td class="num">3,729,010 QPS</td>
<td class="num">2,631,454 QPS</td>
<td class="num">1787.6 MB</td>
<td class="num">1828.2 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">196,209 QPS</td>
<td class="num">509,271 QPS</td>
<td class="num">315,559 QPS</td>
<td class="num">4062.7 MB</td>
<td class="num">1602.7 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">380,683 QPS</td>
<td class="num">1,325,381 QPS</td>
<td class="num">742,633 QPS</td>
<td class="num">3036.4 MB</td>
<td class="num">1225.7 MB</td>
</tr>
<tr>
<td>TinyDBM</td>
<td class="num">7,376,532 QPS</td>
<td class="num">8,314,453 QPS</td>
<td class="num">7,596,498 QPS</td>
<td class="num">3573.0 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>BabyDBM</td>
<td class="num">687,414 QPS</td>
<td class="num">3,481,460 QPS</td>
<td class="num">655,081 QPS</td>
<td class="num">2960.3 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>CacheDBM</td>
<td class="num">6,871,719 QPS</td>
<td class="num">7,358,504 QPS</td>
<td class="num">8,325,958 QPS</td>
<td class="num">4999.5 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdHashDBM</td>
<td class="num">711,054 QPS</td>
<td class="num">10,794,019 QPS</td>
<td class="num">1,334,017 QPS</td>
<td class="num">6409.6 MB</td>
<td class="num">n/a</td>
</tr>
<tr>
<td>StdTreeDBM</td>
<td class="num">103,929 QPS</td>
<td class="num">3,944,576 QPS</td>
<td class="num">121,817 QPS</td>
<td class="num">6595.6 MB</td>
<td class="num">n/a</td>
</tr>
</table>

<p>In general, if you want a key-value storage with the highest performance, choosing the file hash database is recommended.  If you need ordered access of records, choosing the file tree database is recommended.  If you need scalability of ordered databases, choosing the file skip database is recommended.  If you need extreme performance, the on-memory hash database and the on-memory tree database are useful although they require another way to save/load records.  If you want a cache system with deletion of LRU records, the cache database is useful.</p>

<p>If the data structure is more complex than key-value pairs, you can treat the value as a serialized text/binary of any data structure.  You can use any serialization format.  To look up records by some properties except for the primary key, you can use secondary indices.  MemIndex and FileIndex classes are useful for the purpose.</p>

<p>You can use some bridging interfaces to use Tkrzw in other programming langauges than C++.  Currently, Java, Python, and Ruby interfaces are provided.</p>

<p>Command line utilities are provided as well.  Thereby, you can create databases, set records, retrieve records, remove records, rebuild databases, and restore databases.  A tool to do performance tests is also bundled.</p>

<h2 id="download">Download</h2>

<p>You can download source packages in the following directories.</p>

<ul>
<li><a href="http://dbmx.net/tkrzw/pkg/">C++ source packages</a></li>
<li><a href="http://dbmx.net/tkrzw/pkg-java/">Java source packages</a></li>
<li><a href="http://dbmx.net/tkrzw/pkg-python/">Python source packages</a></li>
<li><a href="http://dbmx.net/tkrzw/pkg-ruby/">Ruby source packages</a></li>
</ul>

<h2 id="api_documents">API Documents</h2>

<p>The following are API documents for each language.</p>

<ul>
<li><a href="http://dbmx.net/tkrzw/api/">C++ API documents</a></li>
<li><a href="http://dbmx.net/tkrzw/api-java/">Java API documents</a></li>
<li><a href="http://dbmx.net/tkrzw/api-python/">Python API documents</a></li>
<li><a href="http://dbmx.net/tkrzw/api-ruby/">Ruby API documents</a></li>
</ul>

<h2 id="installation">Installation</h2>

<p>Tkrzw is implemented based on the C++17 standard and POSIX.  To install the library, a Unix-like system (Linux, Mac OS X) and GCC (version 7 or later) are required.  Usually, you will run the following commands.</p>

<pre><code class="language-shell-session"><![CDATA[$ ./configure
$ make
$ make check
$ sudo make install
]]></code></pre>

<p>By default, the library and related files are installed under "/usr/local".</p>

<pre><code class="language-shell-session"><![CDATA[/usr/local/lib/libtkrzw.a
/usr/local/lib/libtkrzw.so
/usr/local/lib/libtkrzw.so.0
...
/usr/local/include/tkrzw_lib_common.h
/usr/local/include/tkrzw_dbm.h
/usr/local/include/tkrzw_dbm_hash.h
...
/usr/local/bin/tkrzw_build_util
/usr/local/bin/tkrzw_dbm_perf
/usr/local/bin/tkrzw_dbm_util
...
]]></code></pre>

<p>Let's check whether the command utilities work properly.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_build_util --version
Tkrzw 0.0.1 (library 0.0.1) on SomeOS (any endian)
$ tkrzw_dbm_util create casket.tkh
$ tkrzw_dbm_util set casket.tkh Michael Mike
$ tkrzw_dbm_util set casket.tkh Anne Nancy
$ tkrzw_dbm_util get casket.tkh Michael
Mike
$ tkrzw_dbm_util get casket.tkh Anne
Nancy
]]></code></pre>

<p>Read <a href="api/">the C++ API documents</a> for usage of the library.  Then, let's build a sample program.  Make a file of the follwing C++ code and save it as "helloworld.cc".</p>

<pre><code class="language-shell-session"><![CDATA[#include "tkrzw_dbm_hash.h"

int main(int argc, char** argv) {
  tkrzw::HashDBM dbm;
  dbm.Open("casket.tkh", true).OrDie();
  dbm.Set("hello", "world").OrDie();
  std::cout << dbm.GetSimple("hello") << std::endl;
  dbm.Close().OrDie();
  return 0;
}
]]></code></pre>

<p>To build an application program, you'll typically run a command like this.  The compiler flag "-std=c++17" is necessary if the default C++ version of your compiler is older than C++17.  If you don't use threading functions in the application code, you can omit the compiler flag "-pthread".  Even so, the linking flag "-lpthread" is necessary because the library uses threading functions.</p>

<pre><code class="language-shell-session"><![CDATA[$ g++ -std=c++17 -pthread -I/usr/local/include \
  -O2 -Wall helloworld.cc \
  -L/usr/local/lib -ltkrzw -lstdc++ -lpthread
]]></code></pre>

<p>The bundled command "tkrzw_build_util" is useful to know necessary CPPFLAGS (flags for the preprocessor), and LDFLAGS (flags for the linker).</p>

<pre><code class="language-shell-session"><![CDATA[$ ./tkrzw_build_util config -i
-I/usr/local/include
$ ./tkrzw_build_util config -l
-L/usr/local/lib -ltkrzw -lstdc++ -lrt -lpthread -lm -lc 
]]></code></pre>

<p>In some environments, you can also use the "pkg-config" command to list up those flags.</p>

<pre><code class="language-shell-session"><![CDATA[$ export PKG_CONFIG_PATH="/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH"
$ pkg-config --cflags-only-I tkrzw
-I/usr/local/include
$ pkg-config --libs tkrzw
-L/usr/local/lib -ltkrzw -lstdc++ -lrt -lpthread -lm -lc
]]></code></pre>

<p>To run the test suite, Google Test is required.  Although testing the library is not necessary for most users, you can do so just in case.</p>

<pre><code class="language-shell-session"><![CDATA[$ make test
$ make testrun
]]></code></pre>

<p>Performance tests described in the overview section can be done by the following commands.</p>

<pre><code class="language-shell-session"><![CDATA[# Sequential access tests by one thread.
$ tkrzw_dbm_perf sequence --dbm hash --path casket.tkh --iter 1000000
$ tkrzw_dbm_perf sequence --dbm tree --path casket.tkt --iter 1000000
$ tkrzw_dbm_perf sequence --dbm skip --path casket.tks --iter 1000000
$ tkrzw_dbm_perf sequence --dbm tiny --iter 1000000
$ tkrzw_dbm_perf sequence --dbm baby --iter 1000000
$ tkrzw_dbm_perf sequence --dbm cache --iter 1000000
$ tkrzw_dbm_perf sequence --dbm stdhash --iter 1000000
$ tkrzw_dbm_perf sequence --dbm stdtree --iter 1000000

# Sequential access tests by ten threads.
$ tkrzw_dbm_perf sequence --dbm hash --path casket.tkh --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm tree --path casket.tkt --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm skip --path casket.tks --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm tiny --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm baby --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm cache --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm stdhash --iter 100000 --threads 10
$ tkrzw_dbm_perf sequence --dbm stdtree --iter 100000 --threads 10

# Large scale random access tests by one thread.
$ tkrzw_dbm_perf sequence --dbm hash --path casket.tkh --iter 100000000 --random_key \
  --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm tree --path casket.tkt --iter 100000000 --random_key \
  --align_pow 12 --buckets 1000000 --max_page_size 4080 --max_cached_pages 400000
$ tkrzw_dbm_perf sequence --dbm skip --path casket.tks --iter 100000000 --random_key \
  --sort_mem_size 1500m --max_cached_records 25000000 --reducer last
$ tkrzw_dbm_perf sequence --dbm tiny --iter 100000000 --random_key --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm baby --iter 100000000 --random_key
$ tkrzw_dbm_perf sequence --dbm cache --iter 100000000 --random_key --cap_rec_num 110000000
$ tkrzw_dbm_perf sequence --dbm stdhash --iter 100000000 --random_key --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm stdtree --iter 100000000 --random_key

# Large scale random access tests by ten threads.
$ tkrzw_dbm_perf sequence --dbm hash --path casket.tkh --iter 10000000 --threads 10 --random_key \
  --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm tree --path casket.tkt --iter 10000000 --threads 10 --random_key \
  --align_pow 12 --buckets 1000000 --max_page_size 4080 --max_cached_pages 400000
$ tkrzw_dbm_perf sequence --dbm skip --path casket.tks --iter 10000000 --threads 10 --random_key \
  --sort_mem_size 1500m --max_cached_records 25000000 --reducer last
$ tkrzw_dbm_perf sequence --dbm tiny --iter 10000000 --random_key --threads 10 --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm baby --iter 10000000 --random_key --threads 10
$ tkrzw_dbm_perf sequence --dbm cache --iter 10000000 --random_key --threads 10 --cap_rec_num 110000000
$ tkrzw_dbm_perf sequence --dbm stdhash --iter 10000000 --random_key --threads 10 --buckets 100000000
$ tkrzw_dbm_perf sequence --dbm stdtree --iter 10000000 --random_key --threads 10
]]></code></pre>

<p>If you pursue the best performance, run "./configure --enable-optarch" for configuration.  If you don't need dynamic linking library, run "./configure --disable-shared".  If you want the utility commands with static linking, run "./configure --enable-static".</p>

<h2 id="hashdbm_overview">HashDBM: The File Hash Database</h2>

<p>The file hash database stores key-value structure in a single file.  It uses a hash table and linked lists of records from buckets.  Therefore, given the number of records N and the number of buckets M, the average time complexity of data retrieval is O(N/M).  If M is large enough, the time complexity can be said O(1).</p>

<p>Thread concurrency is pursued in this implementation.  Only reader-writer locking is applied to each hash bucket.  Therefore, even writer threads which can set or remove records performs in parallel.  Blocking is done only when multiple writers try to update the same record simultaneously.  Reader threads which retrieve records don't block each other even for the same record.</p>

<p>Durability is also pursued.  Updating operations support two modes: in-place and appending.  In the in-place mode, the region of an existing record in the file is re-written to modify the value of the record.  In the appending mode, the region in the file for an existing record is never re-written.  Instead, new data to overwrite the value is appended at the end of the file.  The new data is put at the top of the linked list for the record so that it is detected prior to the old one.  In both modes, even if the hash table is broken for some reason, you can restore the database to salvage record data.  In the in-place mode, the record being updated when brokage happens can be lost but the other records can be restored.  In appending mode, any record can never be lost unless the file system itself breaks.</p>

<p>The hash table is static and not reorganized implicitly.  Thus, if load factor of the hash table is high, the database should be rebuilt explicitly by the application.  Rebuilding the database is effective to resolve fragmentation which can happen in both the in-place mode and the apppending mode.  Rebuilding the database doesn't block other database operations.  In other words, you can retrieve and update records while the database is being rebuilt.</p>

<div class="dbstructure"><img src="hashdbm.svg"/></div>

<h3 id="hashdbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_hash.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  HashDBM dbm;

  // Opens a new database.
  dbm.Open("casket.tkh", true);
  
  // Stores records.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Traverses records.
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->First();
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  // Closes the database.
  dbm.Close();

  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case with performance tuning and thorough error checks.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_hash.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  HashDBM dbm;

  // Tuning parameters.
  // The update mode is in-place writing.
  // Using a 4-byte integer for addressing.
  // Using 3-bit = 8 byte unit alignment.
  // Using 7 buckets for the hash table.
  HashDBM::TuningParameters tuning_params;
  tuning_params.update_mode = HashDBM::UPDATE_IN_PLACE;
  tuning_params.offset_width = 4;
  tuning_params.align_pow = 3;
  tuning_params.num_buckets = 7;
  
  // Opens a new database, OPEN_TRUNCATE means to clear the file content.
  // The result is returned as a Status object.
  Status status = dbm.OpenAdvanced(
      "casket.tkh", true, File::OPEN_TRUNCATE, tuning_params);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Stores records.
  // Bit-or assignment to the status updates the status if the original
  // state is SUCCESS and the new state is an error.
  status |= dbm.Set("foo", "hop");
  status |= dbm.Set("bar", "step");
  status |= dbm.Set("baz", "jump");
  if (status != Status::SUCCESS) {
    // The Set operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.tkh", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Retrieves records.
  // If there was no record, NOT_FOUND_ERROR would be returned.
  std::string value;
  status = dbm.Get("foo", &value);
  if (status == Status::SUCCESS) {
    std::cout << value << std::endl;
  } else {
    std::cerr << "missing: " << status << std::endl;
  }

  // Traverses records.
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  if (iter->First() != Status::SUCCESS) {
    // Failure of the First operation is critical so we stop.
    Die("First failed: ", status);
  }
  while (true) {
    // Retrieves the current record data.
    std::string iter_key, iter_value;
    status = iter->Get(&iter_key, &iter_value);
    if (status == Status::SUCCESS) {
      std::cout << iter_key << ":" << iter_value << std::endl;
    } else {
      // This happens at the end of iteration.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      break;
    }
    // Moves the iterator to the next record.
    status = iter->Next();
    if (status != Status::SUCCESS) {
      // This could happen if another thread removed the current record.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      std::cerr << "missing: " << status << std::endl;
      break;
    }
  }

  // Closes the database.
  // Even if you forgot to close it, the destructor would close it.
  // However, checking the status is a good manner.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  return 0;
}
]]></code></pre>

<p>The file hash database as well as other database classes supports call-back functions to process the value of a record.  The operation is done while the record is locked so that the update looks done atomically.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_hash.h"
#include "tkrzw_str_util.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  HashDBM dbm;

  // Opens a new database,
  dbm.Open("casket.tkh", true, File::OPEN_TRUNCATE);

  // Record processor to count events.
  class Counter : public DBM::RecordProcessor {
   public:
    // Update an existing record.
    virtual std::string_view ProcessFull(std::string_view key, std::string_view value) {
      new_value_ = ToString(StrToInt(value) + 1);
      return new_value_;
    }
    // Register a new record.
    virtual std::string_view ProcessEmpty(std::string_view key) {
      return "1";
    }
   private:
    std::string new_value_;
  };

  // Procedure to count up an event.
  // DBM::IncrementSimple does the same job.
  const auto CountUp = [&](std::string_view name) {
    Counter counter;
    dbm.Process(name, &counter, true);
  };

  // Counts up events.
  CountUp("foo");
  CountUp("foo");
  CountUp("bar");

  // Reports counts.
  std::cout << dbm.GetSimple("foo", "0") << std::endl;
  std::cout << dbm.GetSimple("bar", "0") << std::endl;
  std::cout << dbm.GetSimple("baz", "0") << std::endl;

  // Closes the database.
  dbm.Close();

  return 0;
}
]]></code></pre>

<h3 id="hashdbm_format">Format</h3>

<p>The hash database file is composed of five sections: the metadata section, the bucket section, the free block pool section, the record header section, and the record section.</p>

<p>The metadata section dominates the first 128 bytes of the file.  It contains the following fields.</p>

<dl>
<dt>Magic data</dt>
<dd>From the offset 0.  A string "TkrzwHDB\n" and a training null '\0' character.</dd>
<dt>The package major version</dt>
<dd>From the offset 10.  A 1-byte integer.</dd>
<dt>The package minor version</dt>
<dd>From the offset 11.  A 1-byte integer.</dd>
<dt>The static flags</dt>
<dd>From the offset 12.  A 1-byte integer.</dd>
<dt>The offset width</dt>
<dd>From the offset 13.  A 1-byte integer.</dd>
<dt>The alignment power</dt>
<dd>From the offset 14.  A 1-byte integer.</dd>
<dt>The closure flags</dt>
<dd>From the offset 15.  A 1-byte integer.</dd>
<dt>The number of buckets</dt>
<dd>From the offset 16.  An 8-byte big-endian integer.</dd>
<dt>The number of records</dt>
<dd>From the offset 24.  An 8-byte big-endian integer.</dd>
<dt>The effective data size.</dt>
<dd>From the offset 32.  An 8-byte big-endian integer.</dd>
<dt>The file size</dt>
<dd>From the offset 40.  An 8-byte big-endian integer.</dd>
<dt>The modification date.</dt>
<dd>From the offset 48.  An 8-byte big-endian integer.</dd>
<dt>The database type</dt>
<dd>From the offset 56.  A 4-byte big-endian integer.</dd>
<dt>The opaque data</dt>
<dd>From the offset 64.  Arbitrary string data.</dd>
</dl>

<p>The magic data and the package version data are used for identifying the kind of the file.  The version data indicates the version of the Tkrzw package when the file is created.</p>

<p>The static flags specifies flags which are not changed during lifetime of the database.  The flags must have a bit of 0x1 or 0x2.  0x1 represents the in-place update mode.  0x2 represents the appending update mode.</p>

<p>The offset width specifies how many bytes are used to store an offset value.  Given the offset width W, the maximum value is 2^(W*8).  So, W=3 sets the maximum value 16,777,216 and W=4 sets it 4,294,967,296.  The offset width affects the size of the buckets and the footprint of each record.  The alignment power specifies the alignment of the offset value.  Given the alignment power P, the alignment is 2^P.  So, P=2 sets the alignment 4 and P=3 sets it 8.  The alignment affects the size of space for each record.  The maximum database size is determined as 2^(W*8+P).   The default value of the offset width is 4 and the default value of the alignment power is 3.  So, the maximum database size is 32GiB by default.</p>

<p>When a database is opened in the writable mode, the metadata section is updated immediately to set the closure flag zero and set the modification time to the current UNIX time in microseconds.  If the process crushes without closing the database, the flag and the timestamp helps us detect the incident.  If the writable database is closed normally, the closure flag is set one and the modification date is updated too.</p>

<p>The number of buckets determines how many buckets are used for the hash table.  The number of records indicates the current number of living key-value pairs in the database.  The effective data size indicates the total amount of bytes used for the key and the value data of the living records.  In other words, it doesn't include data for removed data, overwritten data, or other footprints.  This figure is used to calculate storage efficiency.</p>

<p>The database type and the opaque data are used for any purposes by the application.  Modifying them is done in place so it doesn't increase the file size even in the appending mode.  The file tree database uses the opaque data to store its metadata.</p>

<p>The bucket section dominates from the offset 128 to an offset determined by the number of buckets and the offset width.  Given the number of buckets M and the offset width W, the end offset is 128 + M * W.  The default value of the number of buckets is 1,048,583.  Then, which dominates from the offset 128 to 4,194,460.  Each bucket is an integer in big-endian.  If there's no record matching the bucket index, the value is zero.  Otherwise, it indicates the offset of the data of the first record in a linked list of records matching the bucket index.</p>

<p>The record section dominates from an aligned offset after the bucket section to the end of the file.  The start point is equal to or after the figure calculated as 128 + num_buckets * W + 1024 and it is aligned to 4096 and 2^P.  The record section contains multiple record data in sequence.  Each record is serialized in the following format.</p>

<dl>
<dt>The magic data</dt>
<dd>1 byte integer.</dd>
<dt>The child offset</dt>
<dd>A big-endian integer with the offset width.</dd>
<dt>The key size</dt>
<dd>A byte delta encoded integer.</dd>
<dt>The value size</dt>
<dd>A byte delta encoded integer.</dd>
<dt>The padding size</dt>
<dd>A 1-byte integer.</dd>
<dt>The key data</dt>
<dd>Arbitrary string data.</dd>
<dt>The value data</dt>
<dd>Arbitrary string data.</dd>
<dt>The padding data</dt>
<dd>Structural data.</dd>
</dl>

<p>The magic data is 0xFF, 0xFE, or 0xFD.  0xFF indicates that the operation is to set a record.  0xFE indicates that the operation is to remove a record.  0xFD is to do nothing.  The magic data also helps us detect data corruption.</p>

<p>The child offset indicates the offset of the next record data in the linked list of the bucket.  When a new record data of the same bucket is added, the bucket points to the new record data and the new record data points to the top one of the existing linked list, or zero if it doesn't exist.</p>

<p>The key size and the value size are represented in byte delta encoding.  A value between 0 and 127 takes 1 byte.  A value between 128 and 16,383 takes 2 bytes. A value between 16,384 and 2,097,151 takes 3 bytes.  A value between 268,435,456 and 34,359,738,367 takes 4 bytes.</p>

<p>The 1-byte padding size can represent 0 to 255.  If the padding size is 238 (0xEE) or more, the first 4 bytes of the padding data indicates the actual size of the padding data in big endian.  The first byte of the actual padding data is 0xDD and the remaining bytes are 0x00.</p>

<p>With the default 4-byte offset width and small-sized (less than 128 bytes) keys and medium-sized (less than 16384 bytes) values, the footprint for each record is 1 + 4 + 1 + 2 + 1 = 9 bytes.  However, due to alignment, padding bytes can be added.  With the default 8-byte alignment, average padding size is about 4 bytes.</p>

<p>The 16 bytes before the record section is the record header section.  It is composed of "TkrzwREC\n", null '\0' character, a 1-byte integer for the offset width, and a 1-byte integer for the alignment power.  They would be used to recover records even if the header was broken.</p>

<p>The 1008 bytes before the record header section is the free block pool section.  It contains pairs of an offset and a size of each free block.  The offset is a big-endian integer of the offset width.  The size is a big-endian integer of 4 bytes.  The maximum number of pairs is determined as 1008 / (W + 4).  Given the offset width 4, the maximum number is 126.</p>

<h2 id="treedbm_overview">TreeDBM: The File Tree Database</h2>

<p>The file tree database stores key-value structure in a single file.  It uses a multiway balanced tree structure called B+ tree.  Therefore, given the number of records N, the average time complexity of data retrieval is O(log N).  Because records are ordered by the key, range searches including forward matching search are supported.</p>

<p>A B+ tree is composed of nodes which are composed of records and links.  Each node is serialized as a "page", which are stored in the file hash database.  Therefore, the file tree database inherits many characteristics of the file hash database: performance, scalability, and durability.  The file tree database uses a double-layered LRU cache system to reduce frequency of inputs and outputs of pages; Pages which have been repeatedly accessed are stored in the "hot" cache so that they are not wiped out even if many pages are accessed by a few traversal operations, which use the "warm" cache.</p>

<p>Thread concurrency is pursued in this implementation.  Only reader-writer locking is applied to each page.  Therefore, if threads randomly access records, they don't block each other.  If multiple threads access the same page, a writer blocks the others but readers don't block other readers.  The page cache system is managed with slotted mutexes so that multiple threads can access the cache system simultaneously.</p>

<p>If records are accessed sequentially in order of the key, performance of the file tree database can be better than the file hash database because file I/O is done by the page and its frequency is much less.  However, if records are accessed randomly, performance of the file tree database is worse than the file hash database.  Likewise, space efficiency of the file tree database can be better than the file hash database if records are inserted in order.  However, if records are inserted randomly, one thirds of storage space is used for padding.  As with the file hash database, rebuilding the database can reduce the file size and it is done without blocking other threads.</p>

<div class="dbstructure"><img src="treedbm.svg"/></div>

<h3 id="treedbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_tree.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  TreeDBM dbm;

  // Opens a new database.
  dbm.Open("casket.tkt", true);
  
  // Stores records.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Find records by forward matching with "ba".
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->Jump("ba");
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    if (!StrBeginsWith(key, "ba")) break;
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  // Closes the database.
  dbm.Close();

  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case with performance tuning and thorough error checks.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_tree.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  TreeDBM dbm;

  // Tuning parameters.
  // The maximum page size is 2000.
  // The maximum number of branches is 128.
  // The maximum number of cached pages is 20000.
  TreeDBM::TuningParameters tuning_params;
  tuning_params.max_page_size = 2000;
  tuning_params.max_branches = 128;
  tuning_params.max_cached_pages = 20000;
  
  // Opens a new database, OPEN_TRUNCATE means to clear the file content.
  // The result is returned as a Status object.
  Status status = dbm.OpenAdvanced(
      "casket.tkt", true, File::OPEN_TRUNCATE, tuning_params);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Stores records.
  // Bit-or assignment to the status updates the status if the original
  // state is SUCCESS and the new state is an error.
  status |= dbm.Set("foo", "hop");
  status |= dbm.Set("bar", "step");
  status |= dbm.Set("baz", "jump");
  if (status != Status::SUCCESS) {
    // The Set operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.tkt", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Retrieves records.
  // If there was no record, NOT_FOUND_ERROR would be returned.
  std::string value;
  status = dbm.Get("foo", &value);
  if (status == Status::SUCCESS) {
    std::cout << value << std::endl;
  } else {
    std::cerr << "missing: " << status << std::endl;
  }

  // Traverses records.
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  if (iter->First() != Status::SUCCESS) {
    // Failure of the First operation is critical so we stop.
    Die("First failed: ", status);
  }
  while (true) {
    // Retrieves the current record data.
    std::string iter_key, iter_value;
    status = iter->Get(&iter_key, &iter_value);
    if (status == Status::SUCCESS) {
      std::cout << iter_key << ":" << iter_value << std::endl;
    } else {
      // This happens at the end of iteration.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      break;
    }
    // Moves the iterator to the next record.
    status = iter->Next();
    if (status != Status::SUCCESS) {
      // This could happen if another thread removed the current record.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      std::cerr << "missing: " << status << std::endl;
      break;
    }
  }

  // Closes the database.
  // Even if you forgot to close it, the destructor would close it.
  // However, checking the status is a good manner.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  return 0;
}
]]></code></pre>

<p>This is an example to make a dictionary from a TSV file.  The keys are normalized into lower case and forward matching queries are supported.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_tree.h"
#include "tkrzw_file_pos.h"
#include "tkrzw_file_util.h"
#include "tkrzw_str_util.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Create a sample input file.
  const std::string tsv_data =
      "apple\tA red fluit.\n"
      "Apple\tA computer company.\n"
      "applet\tA small application.\n"
      "banana\tA yellow fluit.\n";
  WriteFile("casket.tsv", tsv_data);

  // Creates the database manager.
  TreeDBM dbm;

  // Opens a new database with tuning for a small database.
  tkrzw::TreeDBM::TuningParameters tuning_params;
  tuning_params.align_pow = 6;
  tuning_params.num_buckets = 10000;
  tuning_params.max_page_size = 4000;
  dbm.OpenAdvanced("casket.tkt", true, File::OPEN_TRUNCATE, tuning_params);

  // Opens the input file.
  PositionalParallelFile file;
  file.Open("casket.tsv", false);

  // Record processor to concat values with linefeed.
  class ConcatProcessor : public DBM::RecordProcessor {
   public:
    explicit ConcatProcessor(const std::string& value) : value_(value) {}
    std::string_view ProcessFull(std::string_view key, std::string_view value) override {
      value_ = StrCat(value, "\n", value_);
      return value_;
    }
    std::string_view ProcessEmpty(std::string_view key) override {
      return value_;
    }
   private:
    std::string value_;
  };

  // Read each line of the input file.
  FileReader reader(&file);
  std::string line;
  while (reader.ReadLine(&line) == Status::SUCCESS) {
    line = StrStripLine(line);
    const std::vector<std::string> columns = StrSplit(line, "\t");
    if (columns.size() < 2) continue;
    const std::string key = StrLowerCase(columns[0]);
    ConcatProcessor proc(line);
    dbm.Process(key, &proc, true);
  }

  // Find records by forward matching with "app".
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->Jump("app");
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    if (!StrBeginsWith(key, "app")) break;
    std::cout << "---- " << key << " ----" << std::endl;
    std::cout << value << std::endl;
    iter->Next();
  }

  // Close the input file
  file.Close();

  // Closes the database.
  dbm.Close();

  return 0;
}
]]></code></pre>

<h3 id="treedbm_format">Format</h3>

<p>The tree database file is based on the hash database.  Metadata of B+ tree are stored in the opaque metadata section and nodes of B+ tree are stored as records of the hash database.</p>

<p>The opaque metadata section of the hash database is a space to store an arbitrary 64-byte string, which the tree database uses to store its metadata.  It contains the following fields.</p>

<dl>
<dt>Magic data</dt>
<dd>From the offset 0.  A string "TDB" and a training null '\0' character.</dd>
<dt>The number of records</dt>
<dd>From the offset 4.  A 6-byte big-endian integer.</dd>
<dt>The effective data size.</dt>
<dd>From the offset 10.  A 6-byte big-endian integer.</dd>
<dt>The root page ID</dt>
<dd>From the offset 16.  A 6-byte big-endian integer.</dd>
<dt>The first page ID.</dt>
<dd>From the offset 22.  A 6-byte big-endian integer.</dd>
<dt>The last page ID.</dt>
<dd>From the offset 28.  A 6-byte big-endian integer.</dd>
<dt>The number of leaf nodes</dt>
<dd>From the offset 34.  A 6-byte big-endian integer.</dd>
<dt>The number of inner nodes</dt>
<dd>From the offset 40.  A 6-byte big-endian integer.</dd>
<dt>The maximum page size</dt>
<dd>From the offset 46.  A 3-byte big-endian integer.</dd>
<dt>The maximum number of branches</dt>
<dd>From the offset 49.  A 3-byte big-endian integer.</dd>
<dt>The level of the tree</dt>
<dd>From the offset 52.  A 1-byte integer.</dd>
<dt>The key comparator type</dt>
<dd>From the offset 53.  A 1-byte integer.</dd>
<dt>The opaque data</dt>
<dd>From the offset 54.  Arbitrary string data.</dd>
</dl>

<p>The magic data is used for identifying the kind of the database.  The number of records indicates the current number of living key-value pairs in the database.  The effective data size indicates the total amount of bytes used for the key and the value data of the living records.  In other words, it doesn't include data for removed data, overwritten data, or other footprints.  This figure is used to calculate storage efficiency.</p>

<p>There are two kinds of nodes of B+ tree: leaf nodes and inner nodes.  Leaf nodes contain records of key-value pairs.  Inner nodes contain keys and links to other nodes.  The root ID indicates the ID of the root node of B+ tree.  The root node is used as the entry point of search.  The first ID indicates the ID of the first leaf node of B+ tree.  Leaf nodes are linked in a double linked list and the first leaf node is the entry point of sequential access.  The level of the tree indicates how many layers exists in the tree.  If there's no inner tree, the level is 1.</p>

<p>The key comparator type indicates a function used to compare keys of records.  The default LexicalKeyComparator is 1.  LexicalCaseKeyComparator which ignores case is 2.  DecimalKeyComparator which compares keys as numeric decimal integer expressions is 3.  HexadecimalKeyComparator which compares keys as numeric hexadecimal integer expressions is 4.  RealNumberKeyComparator which compares keys as decimal real number expressions is 5.  Their counterparts for pair strings are 101, 102, 103, and 104.  255 means that another custom comparator is set.  In that case, the comparator must be specified every time when the database is opened.</p>

<p>A Leaf nodes has a key which is a 6-byte big-endian integer of its ID.  The ID is a unique number not less than 1.  Its value is serialized in the following format.  Records are sorted in ascending order of the key.</p>

<dl>
<dt>The previous leaf node ID</dt>
<dd>A 6-byte big-endian integer.</dd>
<dt>The next leaf node ID</dt>
<dd>A 6-byte big-endian integer.</dd>
<dt>Repetition of records</dt>
<dd>Pairs of key-value records.</dd>
<dd>The key is composed of the key size in byte delta encoding and an arbitrary key data.</dd>
<dd>The value is composed of the value size in byte delta encoding and an arbitrary value data.</dd>
</dl>

<p>An inner nodes has a key which is a 6-byte big-endian integer of Its ID.  The ID is a unique number not less than 2^46 * 3.  Its value is serialized in the following format.  links are sorted in ascending order of the key.</p>

<dl>
<dt>The heir node ID</dt>
<dd>A 6-byte big-endian integer.</dd>
<dt>Repetition of links</dt>
<dd>Pairs of key/ID links.</dd>
<dd>The key is composed of the key size in byte delta encoding and an arbitrary key data.</dd>
<dd>The ID is a 6-byte big-endian integer.</dd>
</dl>

<p>By default, the maximum payload size of each leaf node is 8130.  When the page size exceeds it, the leaf node is divided into two and the half size 4065 fits the typical page size 4096 of the operating system.  This maximizes space efficiency.  If a node is divided and there is no parent node, a new inner node is created and the ID of the first divided node becomes the heir ID.  The first key of the second divided node and the ID of the second divided node compose a link, which is inserted in the parent node.  By default, the maximum number of links of each inner node is 256.  When the number of links exceeds it, the inner node is divided into two and their links are inserted to the parent.  This operation is done recursively to the root of the tree.</p>

<p>When the page size of the leaf node becomes less than half of the maximum payload size, the leaf is merged with a sibling node.  Therefore, actually, the page size of the merged leaf node can be 1.5 times of the maximum payload size.  When the number of links of an inner node becomes less than half of the maximum number of links, the inner node is merged to a sibling node.  This operation is done recursively to the root of the tree.  If the root is an inner node and it has only one child, the root is removed and the only child becomes the new root.</p>

<h2 id="skipdbm_overview">SkipDBM: The File Skip Database</h2>

<p>The file skip database stores key-value structure in a single file.  It uses a skip list of sorted records.  Therefore, given the number of records N, the average time complexity of data retrieval is O(log N).  Because records are ordered by the key, range searches including forward matching search are supported.  Retrieving a record by the positional index is also supported.</p>

<p>Although skip list is a static structure, you can modify the database with usual methods of DBM, like the Set and Remove methods.  Updates are reflected on the file in a batch manner.  After a series of updates, you have to synchronize the database by calling the Synchronize method or close the database by the Close method.  Then, you can retrieve records.</p>

<p>The file skip database supports two building modes: at-random and in-order.  In the default at-random mode, records can be inserted in random order.  When the database is synchronized, input records are sorted by merge sort with temporary files.  Thus, you can build a large database whose size exceeds the RAM capacity of your machine.  The average time complexity of building the entire database is O(N * log N) in the at-random mode.  In the in-order mode, records must be inserted in ascending order of the key.  The average time complexity of building the entire database is O(N) in the in-order mode.  In both modes, duplicated keys are allowed but you can call an arbitrary reducer function to solve them later.</p>

<p>Records can be inserted concurrently in the at-random mode although synchronizing the database takes the most of the time for building the database.  In the in-order mode, it is no use to insert records concurrently because the order cannot be assured by concurrent insertion.  For retrieval, only reader locking is applied so that multiple threads can access the database in parallel.</p>

<div class="dbstructure"><img src="skipdbm.svg"/></div>

<h3 id="skipdbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_skip.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  SkipDBM dbm;

  // Opens a new database.
  dbm.Open("casket.tks", true);
  
  // Stores records.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Synchronizes the database.
  dbm.Synchronize(false);

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Traverses records.
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->First();
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  // Closes the database.
  dbm.Close();

  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case with performance tuning and thorough error checks.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_skip.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  SkipDBM dbm;

  // Tuning parameters.
  // Using a 4-byte integer for addressing.
  // Setting 1 in 3 records to have skip links.
  // Setting 8 as the maximum level of the skip list.
  // Assuming the input records are in ascending order.
  SkipDBM::TuningParameters tuning_params;
  tuning_params.offset_width = 4;
  tuning_params.step_unit = 3;
  tuning_params.max_level = 8;
  tuning_params.insert_in_order = true;
  
  // Opens a new database, OPEN_TRUNCATE means to clear the file content.
  // The result is returned as a Status object.
  Status status = dbm.OpenAdvanced(
      "casket.tks", true, File::OPEN_TRUNCATE, tuning_params);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Prepares records in ascending order of the key.
  // The in-order mode requires sorted records.
  std::map<std::string, std::string> input_records;
  input_records["foo"] = "hop";
  input_records["bar"] = "step";
  input_records["baz"] = "jump";

  // Stores records.
  for (const auto& input_record : input_records) {
    status = dbm.Set(input_record.first, input_record.second);
    if (status != Status::SUCCESS) {
      // The Set operation shouldn't fail.  So we stop if it happens.
      Die("Set failed: ", status);
    }
  }

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.tks", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Retrieves records.
  // If there was no record, NOT_FOUND_ERROR would be returned.
  std::string value;
  status = dbm.Get("foo", &value);
  if (status == Status::SUCCESS) {
    std::cout << value << std::endl;
  } else {
    std::cerr << "missing: " << status << std::endl;
  }

  // Traverses records with forward matching with "ba"
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  status = iter->Jump("ba");
  if (status == Status::NOT_FOUND_ERROR) {
    // This could happen if there are no records equal to or greater than it.
    std::cerr << "missing: " << status << std::endl;
  } else if (status != Status::SUCCESS) {
    // Other errors are critical so we stop.
    Die("Jump failed: ", status);
  }
  while (true) {
    // Retrieves the current record data.
    std::string iter_key, iter_value;
    status = iter->Get(&iter_key, &iter_value);
    if (status == Status::SUCCESS) {
      if (!StrBeginsWith(iter_key, "ba")) break;
      std::cout << iter_key << ":" << iter_value << std::endl;
    } else {
      // This happens at the end of iteration.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      break;
    }
    // Moves the iterator to the next record.
    status = iter->Next();
    if (status != Status::SUCCESS) {
      // This could happen if another thread cleared the database.
      if (status != Status::NOT_FOUND_ERROR) {
        // Error types other than NOT_FOUND_ERROR are critical.
        Die("Iterator::Get failed: ", status);
      }
      std::cerr << "missing: " << status << std::endl;
      break;
    }
  }

  // Closes the database.
  // Even if you forgot to close it, the destructor would close it.
  // However, checking the status is a good manner.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  return 0;
}
]]></code></pre>

<p>Although the file skip database is not suitable for incessant updates, it supports updating content by adding some records and merging them with the entire database.  You can apply an arbitrary reducing function to handle records with the same key.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_skip.h"
#include "tkrzw_str_util.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  SkipDBM dbm;

  // Opens a new database,
  Status status = dbm.Open("casket.tks", true, File::OPEN_TRUNCATE);

  // Adds records.
  // Duplicated keys are allowed.
  dbm.Set("Japan", "Tokyo");
  dbm.Set("Japan", "Osaka");
  dbm.Set("France", "Paris");
  dbm.Set("China", "Beijing");

  // Synchronizes the database.
  // This makes new records visible.
  dbm.Synchronize(false);

  // Prints all records.
  PrintL("-- Original Records --");
  PrintDBMRecordsInTSV(&dbm);

  // Adds more records.
  dbm.Set("Japan", "Nagoya");
  dbm.Set("China", "Shanghai");

  // Removes a record.
  dbm.Remove("France");

  // Synchronizes the database.
  // This makes the updates visible.
  dbm.Synchronize(false);

  // Prints all records.
  PrintL("-- Records After Updates --");
  PrintDBMRecordsInTSV(&dbm);

  // Synchronizes the database with a reducer to deduplicate records.
  dbm.SynchronizeAdvanced(false, nullptr, SkipDBM::ReduceToFirst);
  
  // Prints all records.
  PrintL("-- Records After Deduplication --");
  PrintDBMRecordsInTSV(&dbm);

  // Closes the database.
  status = dbm.Close();

  return 0;
}
]]></code></pre>

<h3 id="skipdbm_format">Format</h3>

<p>The skip database file is composed of two sections: the metadata section and the record section.</p>

<p>The metadata section dominates the first 128 bytes of the file.  It contains the following fields.</p>

<dl>
<dt>Magic data</dt>
<dd>From the offset 0.  A string "TkrzwSDB\n" and a training null '\0' character.</dd>
<dt>The package major version</dt>
<dd>From the offset 10.  A 1-byte integer.</dd>
<dt>The package minor version</dt>
<dd>From the offset 11.  A 1-byte integer.</dd>
<dt>The offset width</dt>
<dd>From the offset 12.  A 1-byte integer.</dd>
<dt>The step unit</dt>
<dd>From the offset 13.  A 1-byte integer.</dd>
<dt>The maximum level</dt>
<dd>From the offset 14.  A 1-byte integer.</dd>
<dt>The closure flags</dt>
<dd>From the offset 15.  A 1-byte integer.</dd>
<dt>The number of records</dt>
<dd>From the offset 24.  An 8-byte big-endian integer.</dd>
<dt>The effective data size.</dt>
<dd>From the offset 32.  An 8-byte big-endian integer.</dd>
<dt>The file size</dt>
<dd>From the offset 40.  An 8-byte big-endian integer.</dd>
<dt>The modification date.</dt>
<dd>From the offset 48.  An 8-byte big-endian integer.</dd>
<dt>The database type</dt>
<dd>From the offset 56.  A 4-byte big-endian integer.</dd>
<dt>The opaque data</dt>
<dd>From the offset 64.  Arbitrary string data.</dd>
</dl>

<p>The offset width specifies how many bytes are used to store an offset value.  Given the offset width W, the maximum value is 2^(W*8).  So, W=3 sets the maximum value 16,777,216 and W=4 sets it 4,294,967,296.  The offset width affects the size of the buckets and the footprint of each record.  The default value of the offset width is 4, which means the maximum file size is 4GB.  The step unit specifies how often records are given links to forward records.  Given the step unit S, records whose indices can be divided by S have the level 1 links, which refers to the forward record in S-th steps.  Records whose indices can be divided by S^2 have the level 2 links, which refers to the forward in S^2-th steps.  Likewise, fewer records have higher level links.  The highest level is limited by the maximum level property.  To get the optimal performance, given the number of records N, the maximum level should be higher than Log_S N - 1.  So given, R=1000000 and S=4, the maximum level should be higher than log_4 100000 - 1 = 8.96.  The default value of the step unit is 4 and the default value of the maximum level is 14.  So, they are optimal until the number of records is 4^15 = 1,073,741,824.</p>

<p>When a database is opened in the writable mode, the metadata section is updated immediately to set the closure flag zero and set the modification time to the current UNIX time in microseconds.  If the process crushes without closing the database, the flag and the timestamp helps us detect the incident.  If the writable database is closed normally, the closure flag is set one and the modification date is updated too.</p>

<p>The record section dominates from 128 to the end of the file.  The record section contains multiple record data in sequence in ascending order of the key.  Each record is serialized in the following format.</p>

<dl>
<dt>The magic data</dt>
<dd>1 byte integer.</dd>
<dt>The offsets of the forward records</dt>
<dd>A big-endian integers with the offset width.</dd>
<dt>The key size</dt>
<dd>A byte delta encoded integer.</dd>
<dt>The value size</dt>
<dd>A byte delta encoded integer.</dd>
<dt>The key data</dt>
<dd>Arbitrary string data.</dd>
<dt>The value data</dt>
<dd>Arbitrary string data.</dd>
</dl>

<p>The magic data is 0xFF and helps us detect data corruption.</p>

<p>The offsets of the forward records are composed of big-endian integers whose number is the same as the level of the record.  The level is determined by how many times the index of the record can be divided by the step unit without a remainder.  Given the step unit S and the index C, the destination of the first level link is the record whose index is C + S^1.  The destination of the second level link is the record whose index is C + S^2.</p>

<p>The key size and the value size are represented in byte delta encoding.  A value between 0 and 127 takes 1 byte.  A value between 128 and 16,383 takes 2 bytes. A value between 16,384 and 2,097,151 takes 3 bytes.  A value between 268,435,456 and 34,359,738,367 takes 4 bytes.</p>

<h2 id="tinydbm_overview">TinyDBM: The On-memory Hash Database</h2>

<p>The on-memory hash database stores key-value structure on-memory.  It uses a hash table and linked lists of records from buckets.  Therefore, given the number of records N and the number of buckets M, the average time complexity of data retrieval is O(N/M).  If M is large enough, the time complexity can be said O(1).</p>

<p>Thread concurrency is pursued in this implementation.  Only reader-writer locking is applied to each hash bucket.  Therefore, even writer threads which can set or remove records performs in parallel.  Blocking is done only when multiple writers try to update the same record simultaneously.  Reader threads which retrieve records don't block each other even for the same record.</p>

<p>Whereas thread safety and thread performance is the most important features of the on-memory hash database, memory efficiency is also remarkable.  Because the key and the value, and all metadata are serialized in a single sequence of bytes, memory footprint is minimum.  Typically, pure footprint except for the footprint from the memory allocator is 10 bytes.  Assuming the key and the value are 8-byte strings, actual memory usage is about 55% of std::map&lt;std::string, std::string&gt;.</p>

<p>You don't have to open or close on-memory databases to use them.  You can just set records and retrieve them as if they are std::unordered_map.  However, you can associate the database to a file by opening it with a file path.  Then, when you close the database, all records are saved in the file.  And, you can reuse the records by opening the database with the same file path.</p>

<h3 id="tinydbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_tiny.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // No need to call the Open and Close methods.
  TinyDBM dbm;

  // Stores records.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Traverses records.
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->First();
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case where records are saved in a file and they are reused later.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_tiny.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // The number of buckets can be set by the constructor.
  TinyDBM dbm(11);

  // Opens a database by associating it with a file.
  // The result is returned as a Status object.
  Status status = dbm.Open("casket.flat", true, File::OPEN_TRUNCATE);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Stores records.
  // On-memory databases don't cause errors except for logical ones:
  // NOT_FOUND_ERROR and DUPLICATION_ERROR.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.flat", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Lands all records whose status is "jump".
  auto iter = dbm.MakeIterator();
  iter->First();
  std::string key;
  while (iter->Get(&key) == Status::SUCCESS) {
    if (dbm.CompareExchange(key, "jump", "land") == Status::SUCCESS) {
      std::cout << key << " landed" << std::endl;
    }
    std::cout << key << " is now " << dbm.GetSimple(key) << std::endl;
    iter->Next();
  }

  // Closes the database.
  // In the reader mode, the file is not updated and no error occurrs.
  dbm.Close();

  return 0;
}
]]></code></pre>

<h2 id="babydbm_overview">BabyDBM: The On-memory Tree Database</h2>

<p>The on-memory tree database stores key-value structure on-memory.  It uses a multiway balanced tree structure called B+ tree.  Therefore, given the number of records N, the average time complexity of data retrieval is O(log N).  Because records are ordered by the key, range searches including forward matching search are supported.</p>

<p>A B+ tree is composed of nodes which are composed of records and links.  Each node is serialized as a "page", which are kept on-memory.  Thread concurrency is pursued in this implementation.  Only reader-writer locking is applied to each page. Therefore, if threads randomly access records, they don't block each other. If multiple threads access the same page, a writer blocks the others but readers don't block other readers.</p>

<p>Whereas thread safety and thread performance is the most important features of the on-memory tree database, memory efficiency is also remarkable.  Because the key and the value, and all metadata are serialized in a single sequence of bytes, memory footprint is minimum.  Typically, pure footprint except for the footprint from the memory allocator is 10 bytes.  Assuming the key and the value are 8-byte strings, actual memory usage is about 50% of std::map&lt;std::string, std::string&gt;.</p>

<p>You don't have to open or close on-memory databases to use them.  You can just set records and retrieve them as if they are std::unordered_map.  However, you can associate the database to a file by opening it with a file path.  Then, when you close the database, all records are saved in the file.  And, you can reuse the records by opening the database with the same file path.</p>

<h3 id="babydbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_baby.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // No need to call the Open and Close methods.
  BabyDBM dbm;

  // Stores records.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Find records by forward matching with "ba".
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->Jump("ba");
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    if (!StrBeginsWith(key, "ba")) break;
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case where records are saved in a file and they are reused later.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_baby.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // The key comparator can be set by the constructor.
  BabyDBM dbm(DecimalKeyComparator);

  // Opens a database by associating it with a file.
  // The result is returned as a Status object.
  Status status = dbm.Open("casket.flat", true, File::OPEN_TRUNCATE);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Stores records.
  // On-memory databases don't cause errors except for logical ones:
  // NOT_FOUND_ERROR and DUPLICATION_ERROR.
  dbm.Set("3", "hop");
  dbm.Set("20", "step");
  dbm.Set("100", "jump");

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.flat", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Iterate each record in reverse order.
  auto iter = dbm.MakeIterator();
  iter->Last();
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    std::cout << key << ":" << value << std::endl;
    iter->Previous();
  }

  // Closes the database.
  // In the reader mode, the file is not updated and no error occurrs.
  dbm.Close();

  return 0;
}
]]></code></pre>

<h2 id="cachedbm_overview">CacheDBM: The On-memory Cache Database with LRU Deletion</h2>

<p>The on-memory cache database stores key-value structure on-memory.  It uses a hash table and triple linked lists: from the buckets to each record, from the first record to the last record, and from the last record to the first record.  When a record is accessed, it is placed at the end of the list.  Therefore, the least recent used record is placed at the first position.  The cache database has a capacity to keep the memory usage stable.  When the number of records exceeds the capacity, least recent used records are removed implicitly.  The average time complexity is O(1).</p>

<p>The cache database is sharded internally so that mutex is done for each shard, in order to improve concurrent performance.  The actuall concurrent performance of the cache database is almost the same as the normal on-memory hash database.  Whereas space efficiency of the cache database is worse than those of the on-memory hash database and the on-memory tree dagtabase, it is suitable as a cache system, thanks to the LRU deletion feature.</p>

<p>You don't have to open or close on-memory databases to use them.  You can just set records and retrieve them as if they are std::unordered_map.  However, you can associate the database to a file by opening it with a file path.  Then, when you close the database, all records are saved in the file.  And, you can reuse the records by opening the database with the same file path.</p>

<h3 id="cachedbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_cache.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // No need to call the Open and Close methods.
  // The capacity is set with the constructor.
  constexpr int64_t cap_num_rec = 1000;
  constexpr int64_t cap_mem_size = cap_num_rec * 100;
  CacheDBM dbm(cap_num_rec, cap_mem_size);

  // Stores records.
  for (int32_t i = 0; i < 2000; i++) {
    dbm.Set(ToString(i), ToString(i));
  }

  // Check the number of records records.
  std::cout << "count: " << dbm.CountSimple() << std::endl;

  // Recent records should be alive.
  for (int32_t i = 1990; i < 2000; i++) {
    std::cout << dbm.GetSimple(ToString(i), "*") << std::endl;
  }

  return 0;
}
]]></code></pre>

<h2 id="stddbm_overview">Std(Hash|Tree)DBM: The STL On-memory Databases</h2>

<p>The standard on-memory hash database StdHashDBM stores key-value structure on memory with std::unordered_map.  The standard on-memory tree database stores key-value structure on memory with std::map.  As the hash database uses a hash table, the average time complexity of data retrieval is O(1).  As the tree database uses a binary tree, the average time complexity of data retrieval is O(log N).</p>

<p>Thread safety is assured in these implementation.  Reader-writer locking is applied to the whole database.  Therefore, multiple reader threads can access the same database without blocking whereas a writer thread blocks other threads.  Thread safety is the only benefit of these implementation over using bare std::unordered_map or std::map.</p>

<p>You don't have to open or close on-memory databases to use them.  You can just set records and retrieve them as if they are std::unordered_map and std::map.  However, you can associate the database to a file by opening it with a file path.  Then, when you close the database, all records are saved in the file.  And, you can reuse the records by opening the database with the same file path.</p>

<p>In contrast with the hash database, the tree database StdTreeDBM assures that all keys are ordered in alphabetical order.  The iterator supports the Jump method, which enables range searches including forward-matching search.</p>

<h3 id="hashdbm_example">Example Code</h3>

<p>This is a code example where basic operations are done without checking errors.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_std.h"
#include "tkrzw_str_util.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  // No need to call the Open and Close methods.
  StdTreeDBM dbm;

  // Stores records
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Retrieves records.
  std::cout << dbm.GetSimple("foo", "*") << std::endl;
  std::cout << dbm.GetSimple("bar", "*") << std::endl;
  std::cout << dbm.GetSimple("baz", "*") << std::endl;
  std::cout << dbm.GetSimple("outlier", "*") << std::endl;

  // Find records by forward matching with "ba".
  std::unique_ptr<DBM::Iterator> iter = dbm.MakeIterator();
  iter->Jump("ba");
  std::string key, value;
  while (iter->Get(&key, &value) == Status::SUCCESS) {
    if (!StrBeginsWith(key, "ba")) break;
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }

  return 0;
}
]]></code></pre>

<p>This is a code example which represents a more serious use case where records are saved in a file and they are reused later.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_std.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  StdTreeDBM dbm;

  // Opens a database by associating it with a file.
  // The result is returned as a Status object.
  Status status = dbm.Open("casket.flat", true, File::OPEN_TRUNCATE);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }
  
  // Stores records.
  // On-memory databases don't cause errors except for logical ones:
  // NOT_FOUND_ERROR and DUPLICATION_ERROR.
  dbm.Set("foo", "hop");
  dbm.Set("bar", "step");
  dbm.Set("baz", "jump");

  // Closes the database.
  status = dbm.Close();
  if (status != Status::SUCCESS) {
    // The Close operation shouldn't fail.  So we stop if it happens.
    Die("Set failed: ", status);
  }

  // Opens the exisiting database as a reader mode.
  status = dbm.Open("casket.flat", false);
  if (status != Status::SUCCESS) {
    // Failure of the Open operation is critical so we stop.
    Die("Open failed: ", status);
  }

  // Prints all records atomically.
  class Printer : public DBM::RecordProcessor {
   public:
    std::string_view ProcessFull(std::string_view key, std::string_view value) override {
      std::cout << key << ":" << value << std::endl;
      return NOOP;
    }
  } printer;
  dbm.ProcessEach(&printer, false);

  // Closes the database.
  // In the reader mode, the file is not updated and no error occurrs.
  dbm.Close();

  return 0;
}
]]></code></pre>


<h3 id="polydbm_overview">(Poly|Shard)DBM: Polymorphic and Sharding DBM Adapters</h3>

<p>Although all database classes share the same interface defined by the DBM class, constructors and tuning parameters are different.  The polymorphic database manager adapter PolyDBM is provided in order to absorb such difference so that you can always use the same class PolyDBM.  The concrete class is determined by the extension of the path.  You can open a hash database by the following code.</p>

<pre><code class="language-cpp"><![CDATA[PolyDBM dbm;
dbm.Open("casket.tkh", true);
]]></code></pre>

<p>The following extensions are recognizable.</p>

<ul>
<li>HashDBM: <code>.tkh</code>, <code>.hash</code></li>
<li>TreeDBM: <code>.tkt</code>, <code>.tree</code></li>
<li>SkipDBM: <code>.tks</code>, <code>.skip</code></li>
<li>TinyDBM: <code>.tkmt</code>, <code>.tiny</code>, <code>.flat</code></li>
<li>BabyDBM: <code>.tkmb</code>, <code>.baby</code></li>
<li>CacheDBM: <code>.tkmc</code>, <code>.cache</code></li>
<li>StdHashDBM: <code>.tksh</code>, <code>.stdhash</code></li>
<li>StdTreeDBM: <code>.tkst</code>, <code>.stdtree</code></li>
</ul>

<p>You can specify the class name directly by passing the "dbm" parameter to the OpenAdvanced method.  It also takes tuning parameters.  You can use the hash database and tune it by the following code.</p>

<pre><code class="language-cpp"><![CDATA[PolyDBM dbm;
const std::map<std::string, std::string> params = {
  {"dbm", "HashDBM"}, {"align_pow", "0"}, {"num_buckets", "10000"}};
dbm.OpenAdvanced("casket", true, File::OPEN_DEFAULT, params);
]]></code></pre>

<p>On-memory databases also have to be opened by the Open or OpenAdvanced method.  However, if the path is empty, records are not saved in or loaded from any file.  If you specify a non-empty path, records are saved in and loaded from the file.  The following code opens an on-memory tree database without associating it to any file.</p>

<pre><code class="language-cpp"><![CDATA[PolyDBM dbm;
const std::map<std::string, std::string> params = {
  {"dbm", "BabyDBM"}, {"key_comparator", "DecimalKeyComparator"}};
dbm.OpenAdvanced("", true, File::OPEN_DEFAULT, params);
]]></code></pre>

<p>The polymorphic database manager adapter is also useful when you develop interfaces for other programming languages, like Java, Python, Ruby, Go, Lua etc.  Whereas writing code for each database class is tedious, just writing code for PolyDBM allows you to use all database classes.</p>

<p>The sharding datbase manager adapter ShardDBM is a wrapper of multiple PolyDBM instances for sharding a database.  Whereas an instance of ShardDBM behaves like a usual DBM, records are stored in multiple database files, which are called shards.  A hash function is used to determine which shard each record belongs to.</p>

<p>By separating one database file into several files, concurrent performance improves.  Possibility that a thread is blocked by mutexes is divided by the number of shards.  Moreover, efficiency of rebuilding database improves.  Usually, rebuilding a database file requires making a temporary file whose size is proportional to the original database file.  With sharding, rebuilding database is done for each shard one by one.  Thus, the size of each temporary file is also divided.</p>

<p>ShardDBM can shard any database, whether it is ordered or unordered.  With an ordered database, the iterator gathers records from each shard using a heap tree structure so that records are retrieved in ascending order of the key.</p>

<p>You specify the number of shards by passing the "num_shards" parameter to the OpenAdvanced method.  It also takes the same tuning parameters as PolyDBM.</p>

<pre><code class="language-cpp"><![CDATA[ShardDBM dbm;
const std::map<std::string, std::string> params = {
  {"num_shards", "4"}, {"dbm", "TreeDBM"},
dbm.OpenAdvanced("casket", true, File::OPEN_DEFAULT, params);
]]></code></pre>

<p>If the file path is "casket" and the number of shards is 4, "casket-00000-of-00004", "casket-00001-of-00004", "casket-00002-of-00004", and "casket-00003-of-00004" are created.  To open an existing database, specify the path without the suffix.  You can omit the "num_shards" parameter to open existing files.</p>

<h3 id="polydbm_example">Example Code</h3>

<p>This is a code example of basic usase of PolyDBM.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_poly.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  PolyDBM dbm;

  // Opens a new database.
  const std::map<std::string, std::string> open_params = {
    {"update_mode", "UPDATE_APPENDING"},
    {"max_page_size", "1000"}, {"max_branches", "128"},
    {"key_comparator", "DecimalKeyComparator"},
  };
  dbm.OpenAdvanced("casket.tkt", true, tkrzw::File::OPEN_TRUNCATE, open_params).OrDie();
  
  // Stores records.
  dbm.Set("1", "one").OrDie();
  dbm.Set("2", "two").OrDie();
  dbm.Set("3", "three").OrDie();

  // Rebuild the database.
  const std::map<std::string, std::string> rebuild_params = {
    {"update_mode", "UPDATE_IN_PLACE"},
    {"max_page_size", "4080"}, {"max_branches", "256"},
  };
  dbm.RebuildAdvanced(rebuild_params).OrDie();

  // Retrieves records.
  std::cout << dbm.GetSimple("1", "*") << std::endl;
  std::cout << dbm.GetSimple("2", "*") << std::endl;
  std::cout << dbm.GetSimple("3", "*") << std::endl;
  std::cout << dbm.GetSimple("4", "*") << std::endl;
  
  // Closes the database.
  dbm.Close().OrDie();

  return 0;
}
]]></code></pre>

<p>This is a code example of basic usase of ShardDBM.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_dbm_shard.h"
#include "tkrzw_str_util.h"

// Main routine.
int main(int argc, char** argv) {
  // All symbols of Tkrzw are under the namespace "tkrzw".
  using namespace tkrzw;

  // Creates the database manager.
  ShardDBM dbm;

  // Opens a new database.
  const std::map<std::string, std::string> open_params = {
    {"num_shards", "10"},
    {"dbm", "TreeDBM"}, {"key_comparator", "DecimalKeyComparator"},
  };
  dbm.OpenAdvanced("casket", true, tkrzw::File::OPEN_TRUNCATE, open_params).OrDie();
  
  // Stores records.
  for (int32_t i = 1; i <= 100; i++) {
    const std::string key = tkrzw::ToString(i);
    const std::string value = tkrzw::ToString(i * i);
    dbm.Set(key, value).OrDie();
  }

  // Retrieve records whose keys are 50 or more.
  auto iter = dbm.MakeIterator();
  iter->Jump("50");
  std::string key, value;
  while (iter->Get(&key, &value).IsOK()) {
    std::cout << key << ":" << value << std::endl;
    iter->Next();
  }
  
  // Closes the database.
  dbm.Close().OrDie();

  return 0;
}
]]></code></pre>

<h2 id="index_overview">(File|Mem)Index: Secondary Indices</h2>

<p>HashDBM is the fastest file database in theory and in practice.  Its performance is stable and it tolerates frequent and concurrent updates.  However, it is an unordered database so it supports only exact matching of the key.  In other words, it doesn't support range search or forward matching search, which ordered databases support.  Moreover, compared with relational databases, the interface of DBM is simple and it can find records only by the primary key.  However, combining HashDBM and secondary indices is a practical solution to demands for high performance and complex structure.</p>

<p>You store the main entity data with HashDBM so that you get performance and durability.  The key is a unique string which is considered as the primary key of a relational database.  The value is a serialized complex data structure in a format like JSON and Protocol Buffers.  You make on-memory indices for some fields of the main entity so that you can search the database for records whose specific fields match some conditions.</p>

<p>Typically, an index means an inverted structure made from the main database.  The key of the index is composed of two factors: pattern for matching and the primary key of a record in the main database.  Let's say, you build a database for employees.  The primary key is the employee ID so the main database uses the employee ID as the key.  The value is a data structure composed of a personal name, a division ID, a job title, salary etc.  You make an index for division IDs because you want to know who are working for each division quickly without scanning the whole database for each query.  Then, each record of the index should be composed of a division ID and an employee ID.</p>

<p>FileIndex is a class to handle a secondary index on file.  It is implemented with the file tree database.  MemIndex is a class to handle a secondary index on memory.  It is implemented with the on-memory tree database.  And, StdIndex is a template class to realize secondary indices by wrapping std::set&lt;std::pair&lt;x, Y&gt;&gt; composed of arbitrary data types.</p>

<p>For the division ID index, the first element can be int32_t for the division ID and the second element can be int32_t for the employee ID.  Therefore, using StdIndex&lt;int32_t, int32_t&gt; is a good idea.  By having the primary key of the main database as the second element, each record becomes unique and you don't have to worry about duplication.  You can also set an arbitrary comparator to adjust the order of records.</p>

<p>Secondary indices are not important data to be perpetuated on the storage because they can be reproduced as long as the main entity data exist.  Therefore, using some of memory indices is a practial solution to realize extreme throughput.  To use on-memory indices, you have to load all index entries from the main database every time when the process starts.</p>

<h3 id="index_example">Example Code</h3>

<p>This code represents an advanced usage of HashDBM with FileIndex, which allows for find employees by the division ID.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_hash.h"
#include "tkrzw_index.h"
#include "tkrzw_str_util.h"

// All symbols of Tkrzw are under the namespace "tkrzw".
using namespace tkrzw;

// Structure for an employee.
// Acutually, you should use a serious implementaton like Protocol Buffers.
struct Employee {
  int32_t employee_id = 0;
  std::string name;
  int32_t division_id = 0;
  std::string Serialize() const {
    return StrCat(employee_id, "\t", name, "\t", division_id);
  }
  bool Deserialize(const std::string_view& serialized) {
    const std::vector<std::string> fields = StrSplit(serialized, "\t");
    if (fields.size() < 3) {
      Die("Deserialize failed");
    }
    employee_id = StrToInt(fields[0]);
    name = fields[1];
    division_id = StrToInt(fields[2]);
    return true;
  }
  void Print() {
    std::cout << "employee_id=" << employee_id << " name=" << name
              << " division_id=" << division_id << std::endl;
  }
};

// Updates information of an employee.
// If the name is empty, the entry is removed.
void UpdateEmployee(const Employee& employee, DBM* dbm, FileIndex* division_index) {
  const std::string& primary_key = ToString(employee.employee_id);
  class Updater : public DBM::RecordProcessor {
   public:
    Updater(const Employee& employee, FileIndex* division_index)
        : employee_(employee), division_index_(division_index) {
    }
    // This is called if there is an existing record of the key.
    std::string_view ProcessFull(std::string_view key, std::string_view value) override {
      Employee old_record;
      old_record.Deserialize(value);
      // Removes an entry for the existing record.
      if (old_record.division_id > 0) {
        division_index_->Remove(ToString(old_record.division_id), key);
      }
      // If the new name is empty, removes the record.
      if (employee_.name.empty()) {
        return REMOVE;
      }
      // Adds an entry for the new record.
      if (employee_.division_id > 0) {
        division_index_->Add(ToString(employee_.division_id), key);
      }
      // Updates the record.
      new_value_ = employee_.Serialize();
      return new_value_;
    }
    // This is called if there is no existing record of the key.
    std::string_view ProcessEmpty(std::string_view key) override {
      // If the new name is empty, nothing is done.
      if (employee_.name.empty()) {
        return NOOP;
      }
      // Adds an entry for the new record.
      if (employee_.division_id > 0) {
        division_index_->Add(ToString(employee_.division_id), key);
      }
      // Updates the record.
      new_value_ = employee_.Serialize();
      return new_value_;
    }
   private:
    const Employee& employee_;
    FileIndex* division_index_;
    std::string new_value_;
  } updater(employee, division_index);
  dbm->Process(primary_key, &updater, true).OrDie();
}

// Main routine.
int main(int argc, char** argv) {

  // Creates the database manager.
  HashDBM dbm;

  // Prepare an index for divisions and their members.
  FileIndex division_index;

  // Opens a new database,
  dbm.Open("casket.tkh", true, File::OPEN_TRUNCATE).OrDie();

  // Opens a new index.
  // Setting PairDecimalKeyComparator is to sort the division ID in numeric order.
  // Otherwise, "10" would come earlier than "2" in lexical order.
  TreeDBM::TuningParameters division_index_params;
  division_index_params.key_comparator = PairDecimalKeyComparator;
  Status status = division_index.Open(
      "casket-division-index.tkt", true, File::OPEN_TRUNCATE, division_index_params);

  // Register employee records.
  const std::vector<Employee> employees = {
    {10001, "Anne", 301}, {10002, "Marilla", 301}, {10003, "Matthew", 301},
    {10004, "Diana", 401},
    {10005, "Gilbert", 501},
  };
  for (const Employee& employee : employees) {
    UpdateEmployee(employee, &dbm, &division_index);
  }

  // Closes the index.
  division_index.Close().OrDie();
  
  // Closes the database.
  dbm.Close().OrDie();

  // Opens an existing database,
  dbm.Open("casket.tkh", true).OrDie();

  // Opens an existing index,
  division_index.Open("casket-division-index.tkt", true).OrDie();

  // Updates for employees.
  const std::vector<Employee> updates = {
    {10001, "Anne", 501},  // Anne works with Gilbert.
    {10003, "", 0},        // Matthew left the company.
    {10006, "Minnie May", 401},
    {10007, "Davy", 301}, {10008, "Dora", 301},
  };
  for (const Employee& employee : updates) {
    UpdateEmployee(employee, &dbm, &division_index);
  }

  // Prints members for each division.
  for (const int32_t division_id : {301, 401, 501}) {
    std::cout << "-- Division " << division_id << "  --" << std::endl;
    for (const std::string employee_id :
             division_index.GetValues(ToString(division_id))) {
      Employee employee;
      employee.Deserialize(dbm.GetSimple(employee_id));
      employee.Print();
    }
  }

  // Closes the index.
  division_index.Close().OrDie();

  // Closes the database.
  dbm.Close().OrDie();

  return 0;
}
]]></code></pre>

<p>Let's do the same thing as the above with StdIndex instead of FileIndex.  We scans the main database when the process starts in order to build the on-memory secondary index.</p>

<pre><code class="language-cpp"><![CDATA[#include "tkrzw_cmd_util.h"
#include "tkrzw_dbm_hash.h"
#include "tkrzw_index.h"
#include "tkrzw_str_util.h"

// All symbols of Tkrzw are under the namespace "tkrzw".
using namespace tkrzw;

// Defines an index by the division ID to member employee IDs.
typedef StdIndex<int32_t, int32_t> DivisionIndex;

// Structure for an employee.
// Acutually, you should use a serious implementaton like Protocol Buffers.
struct Employee {
  int32_t employee_id = 0;
  std::string name;
  int32_t division_id = 0;
  std::string Serialize() const {
    return StrCat(employee_id, "\t", name, "\t", division_id);
  }
  void Deserialize(const std::string_view& serialized) {
    const std::vector<std::string> fields = StrSplit(serialized, "\t");
    if (fields.size() < 3) {
      Die("Deserialize failed");
    }
    employee_id = StrToInt(fields[0]);
    name = fields[1];
    division_id = StrToInt(fields[2]);
  }
  void Print() {
    std::cout << "employee_id=" << employee_id << " name=" << name
              << " division_id=" << division_id << std::endl;
  }
};

// Loads index records from the main database.
void LoadIndexRecords(DBM* dbm, DivisionIndex* division_index) {
  class IndexBuilder : public DBM::RecordProcessor {
   public:
    explicit IndexBuilder(DivisionIndex* division_index)
        : division_index_(division_index) {}
    // This is called for each existing record.
    std::string_view ProcessFull(std::string_view key, std::string_view value) override {
      const int32_t key_num = StrToInt(key);
      Employee employee;
      employee.Deserialize(value);
      division_index_->Add(employee.division_id, key_num);
      return NOOP;
    }
   private:
    DivisionIndex* division_index_;
  } index_builder(division_index);
  dbm->ProcessEach(&index_builder, false).OrDie();
}

// Updates information of an employee.
// If the name is empty, the entry is removed.
void UpdateEmployee(const Employee& employee, DBM* dbm, DivisionIndex* division_index) {
  const std::string& primary_key = ToString(employee.employee_id);
  class Updater : public DBM::RecordProcessor {
   public:
    Updater(const Employee& employee, DivisionIndex* division_index)
        : employee_(employee), division_index_(division_index) {
    }
    // This is called if there is an existing record of the key.
    std::string_view ProcessFull(std::string_view key, std::string_view value) override {
      const int32_t key_num = StrToInt(key);
      Employee old_record;
      old_record.Deserialize(value);
      // Removes an entry for the existing record.
      if (old_record.division_id > 0) {
        division_index_->Remove(old_record.division_id, key_num);
      }
      // If the new name is empty, removes the record.
      if (employee_.name.empty()) {
        return REMOVE;
      }
      // Adds an entry for the new record.
      if (employee_.division_id > 0) {
        division_index_->Add(employee_.division_id, key_num);
      }
      // Updates the record.
      new_value_ = employee_.Serialize();
      return new_value_;
    }
    // This is called if there is no existing record of the key.
    std::string_view ProcessEmpty(std::string_view key) override {
      const int32_t key_num = StrToInt(key);
      // If the new name is empty, nothing is done.
      if (employee_.name.empty()) {
        return NOOP;
      }
      // Adds an entry for the new record.
      if (employee_.division_id > 0) {
        division_index_->Add(employee_.division_id, key_num);
      }
      // Updates the record.
      new_value_ = employee_.Serialize();
      return new_value_;
    }
   private:
    const Employee& employee_;
    DivisionIndex* division_index_;
    std::string new_value_;
  } updater(employee, division_index);
  dbm->Process(primary_key, &updater, true).OrDie();
}

// Main routine.
int main(int argc, char** argv) {

  // Creates the database manager.
  HashDBM dbm;

  // Prepare an index for divisions and their members.
  DivisionIndex division_index;

  // Opens a new database,
  dbm.Open("casket.tkh", true, File::OPEN_TRUNCATE).OrDie();

  // After opening the database, indices should be loaded.
  // As the database is empty at this point, there's no effect here.
  LoadIndexRecords(&dbm, &division_index);

  // Register employee records.
  const std::vector<Employee> employees = {
    {10001, "Anne", 301}, {10002, "Marilla", 301}, {10003, "Matthew", 301},
    {10004, "Diana", 401},
    {10005, "Gilbert", 501},
  };
  for (const Employee& employee : employees) {
    UpdateEmployee(employee, &dbm, &division_index);
  }

  // Closes the database.
  dbm.Close().OrDie();

  // Opens an existing database,
  dbm.Open("casket.tkh", true).OrDie();

  // After opening the database, indices should be loaded.
  // All existing records are reflected on the index.
  LoadIndexRecords(&dbm, &division_index);

  // Updates for employees.
  const std::vector<Employee> updates = {
    {10001, "Anne", 501},  // Anne works with Gilbert.
    {10003, "", 0},        // Matthew left the company.
    {10006, "Minnie May", 401},
    {10007, "Davy", 301}, {10008, "Dora", 301},
  };
  for (const Employee& employee : updates) {
    UpdateEmployee(employee, &dbm, &division_index);
  }

  // Prints members for each division.
  for (const int32_t division_id : {301, 401, 501}) {
    std::cout << "-- Division " << division_id << "  --" << std::endl;
    for (int32_t employee_id : division_index.GetValues(division_id)) {
      Employee employee;
      employee.Deserialize(dbm.GetSimple(ToString(employee_id)));
      employee.Print();
    }
  }

  // Closes the database.
  dbm.Close().OrDie();

  return 0;
}
]]></code></pre>

<h2 id="commands">Command Line Utilitie</h2>

<h3 id="commands_tkrzw_build_util">tkrzw_build_util: Build Utilitie</h3>

<p>tkrzw_build_util is a command for build utilities.  Thereby, you can obtain flags to build your own program using the library of Tkrzw.  The usage is the following.  A subcommand name comes as the first argument.  Some options and other arguments can follow.</p>

<dl>
<dt><code>tkrzw_build_util config [<var>options</var>]</code></dt>
<dd>Prints configurations.</dd>
<dt><code>tkrzw_build_util version</code></dt>
<dd>Prints the version information.</dd>
</dl>

<dl>
<dt>Options of the config subcommand:</dt>
<dd><code>-v</code> : Prints the version number.</dd>
<dd><code>-i</code> : Prints C++ preprocessor options for build.</dd>
<dd><code>-l</code> : Prints linker options for build.</dd>
<dd><code>-p</code> : Prints the prefix for installation.</dd>
</dl>

<p>The following is a way to build your own application without setting complicated flags.</p>

<pre><code class="language-shell-session"><![CDATA[$ g++ -std=c++17 -pthread \
  $(tkrzw_build_util config -i) \
  -O2 -Wall helloworld.cc \
  $(tkrzw_build_util config -l)
]]></code></pre>

<h3 id="commands_tkrzw_dbm_util">tkrzw_dbm_util: DBM Utilities</h3>

<p>tkrzw_dbm_util is a command for managing database files.  Thereby, you can create databases, set records, retrieve records, remove records, rebuild databases, and restore databases.  The usage is the following.  A subcommand name comes as the first argument.  Some options and other arguments can follow.</p>

<dl>
<dt><code>tkrzw_dbm_util create [<var>common_options</var>] [<var>tuning_options</var>] [<var>options</var>] <var>file</var></code></dt>
<dd>Creates a database file.</dd>
<dt><code>tkrzw_dbm_util inspect [<var>common_options</var>] <var>file</var></code></dt>
<dd>Prints inspection of a database file.</dd>
<dt><code>tkrzw_dbm_util get [<var>common_options</var>] <var>file</var> <var>key</var></code></dt>
<dd>Gets a record and prints it.</dd>
<dt><code>tkrzw_dbm_util set [<var>common_options</var>] [<var>options</var>] <var>file</var> <var>key</var></code></dt>
<dd>Sets a record.</dd>
<dt><code>tkrzw_dbm_util remove [<var>common_options</var>] <var>file</var> <var>key</var></code></dt>
<dd>Removes a record.</dd>
<dt><code>tkrzw_dbm_util list [<var>common_options</var>] <var>file</var></code></dt>
<dd>Lists up records and prints them.</dd>
<dt><code>tkrzw_dbm_util rebuild [<var>common_options</var>] [<var>tuning_options</var>] <var>file</var></code></dt>
<dd>Rebuilds a database file for optimization.</dd>
<dt><code>tkrzw_dbm_util restore [<var>common_options</var>] <var>old_file</var> <var>new_file</var></code></dt>
<dd>Restores a broken database file.</dd>
<dt><code>tkrzw_dbm_util merge [<var>common_options</var>] <var>dest_file</var> <var>src_files</var>...</code></dt>
<dd>Merges database files.</dd>
<dt><code>tkrzw_dbm_util export [<var>common_options</var>] [<var>options</var>] <var>dbm_file</var> <var>rec_file</var></code></dt>
<dd>Exports records to a flat record file.</dd>
<dt><code>tkrzw_dbm_util import [<var>common_options</var>] [<var>options</var>] <var>dbm_file</var> <var>rec_file</var></code></dt>
<dd>Imports records from a flat record file.</dd>
</dl>

<dl>
<dt>Common options:</dt>
<dd><code>--dbm <var>impl</var></code> : The name of a DBM implementation: auto, hash, tree, skip, tiny, baby, cache, stdhash, stdtree, poly, shard. (default: auto)</dd>
<dd><code>--file <var>impl</var></code> : The name of a file implementation: mmap-para, mmap-atom, pos-para, pos-atom. (default: mmap-para)</dd>
<dd><code>--no_wait</code> : Fails if the file is locked by another process.</dd>
<dd><code>--no_lock</code> : Omits file locking.</dd>
<dt>Options for the create subcommand:</dt>
<dd><code>--truncate</code> : Truncates an existing database file.</dd>
<dt>Options for the set subcommand:</dt>
<dd><code>--no_overwrite</code> : Fails if there's an existing record wit the same key.</dd>
<dd><code>--reducer <var>func</var></code> : Sets the reducer for the skip database: none, first, second, last, concat, concatnull, concattab, concatline, total. (default: none)</dd>
<dt>Options for the list subcommand:</dt>
<dd><code>--jump <var>pattern</var></code> : Jumps to the position of the pattern.</dd>
<dd><code>--items <var>num</var></code> : The number of items to print.</dd>
<dt>Options for the rebuild subcommand:</dt>
<dd><code>--restore</code> : Skips broken records to restore a broken database.</dd>
<dt>Options for the merge subcommand:</dt>
<dd><code>--reducer <var>func</var></code> : Sets the reducer for the skip database: none, first, second, last, concat, concatnull, concattab, concatline, total. (default: none)</dd>
<dt>Options for the restore subcommand:</dt>
<dd><code>--end_offset</code> : The exclusive end offset of records to read. (default: -1)</dd>
<dt>Options for the export and import subcommands:</dt>
<dd><code>--tsv</code> : The record file is in TSV format instead of flat record.</dd>
<dd><code>--escape</code> : C-style escape/unescape is applied to the TSV data.</dd>
<dd><code>--keys</code> : Exports keys only.</dd>
<dt>Tuning options for HashDBM:</dt>
<dd><code>--in_place</code> : Uses in-place rather than pre-defined ones.</dd>
<dd><code>--append</code> : Uses the appending mode rather than the in-place mode.</dd>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4 or -1)</dd>
<dd><code>--align_pow <var>num</var></code> : Sets the power to align records. (default: 3 or -1)</dd>
<dd><code>--buckets <var>num</var></code> : Sets the number of buckets for hashing. (default: 1048583 or -1)</dd>
<dt>Tuning options for TreeDBM:</dt>
<dd><code>--in_place</code> : Uses in-place rather than pre-defined ones.</dd>
<dd><code>--append</code> : Uses appending rather than pre-defined ones.</dd>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4 or -1)</dd>
<dd><code>--align_pow <var>num</var></code> : Sets the power to align records. (default: 10 or -1)</dd>
<dd><code>--buckets <var>num</var></code> : Sets the number of buckets for hashing. (default: 131101 or -1)</dd>
<dd><code>--max_page_size <var>num</var></code> : Sets the maximum size of a page. (default: 8130 or -1)</dd>
<dd><code>--max_branches <var>num</var></code> : Sets the maximum number of branches of inner nodes. (default: 256 or -1)</dd>
<dd><code>--comparator <var>func</var></code> : Sets the key comparator: lex, lexcase, dec, hex, real. (default: lex)</dd>
<dt>Tuning options for SkipDBM:</dt>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4)</dd>
<dd><code>--step_unit <var>num</var></code> : Sets the step unit of the skip list. (default: 4)</dd>
<dd><code>--max_level <var>num</var></code> : Sets the maximum level of the skip list. (default: 14)</dd>
<dd><code>--sort_mem_size <var>num</var></code> : Sets the memory size used for sorting. (default: 268435456)</dd>
<dd><code>--insert_in_order</code> : Inserts records in ascending order order of the key.</dd>
<dt>Options for PolyDBM and ShardDBM:</dt>
<dd><code>--params <var>str</var></code> : Sets the parameters in "key=value,key=value" format.</dd>
</dl>

<p>The following is a sample usage to make a hash database file to associate country names to their capital citys' names.  If the extension of the file is "tkh", the type of the database is regarded as HashDBM.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --buckets 10 casket.tkh
$ tkrzw_dbm_util set casket.tkh Japan Tokyo
$ tkrzw_dbm_util set casket.tkh Korea Seoul
$ tkrzw_dbm_util set casket.tkh China Beijing
$ tkrzw_dbm_util get casket.tkh Japan
Tokyo
$ tkrzw_dbm_util remove casket.tkh Japan
$ tkrzw_dbm_util rebuild casket.tkh
$ tkrzw_dbm_util list casket.tkh
China  Beijing
Korea  Seoul
]]></code></pre>

<p>Let's say, you have a dictionary in TSV format.  The first field is an index word and the second field is its description.  Then, make a skip database of the dictionary.  If the extension of the file is "tks", the type of the database is regarded as SkipDBM.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util import --tsv casket.tks dictionary.tsv
]]></code></pre>

<p>You can look up words which forward-match a pattern.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util list --jump appl --max 3 casket.tks
applaud To praise by clapping.
apple   A delicious red fruit.
applet  A small application.
]]></code></pre>

<h3 id="commands_tkrzw_dbm_perf">tkrzw_dbm_perf: DBM Performance Checker</h3>

<p>tkrzw_dbm_perf is a command for checking performance of DBM implementations.  Thereby, you can check performance of database operations in various scenarios including multithreading.  The usage is the following.  A subcommand name comes as the first argument.  Some options and other arguments can follow.</p>

<dl>
<dt><code>tkrzw_dbm_perf sequence [<var>options</var>]</code></dt>
<dd>Checks setting/getting/removing performance in sequence.</dd>
<dt><code>tkrzw_dbm_perf parallel [<var>options</var>]</code></dt>
<dd>Checks setting/getting/removing performance in parallel.</dd>
<dt><code>tkrzw_dbm_perf wicked [<var>options</var>]</code></dt>
<dd>Checks consistency with various operations.</dd>
<dt><code>tkrzw_dbm_perf index [<var>options</var>]</code></dt>
<dd>Checks performance of on-memory indexing.</dd>
</dl>

<dl>
<dt>Common options:</dt>
<dd><code>--dbm <var>impl</var></code> : The name of a DBM implementation: auto, hash, tree, skip, tiny, baby, cache, stdhash, stdtree, poly, shard. (default: auto)</dd>
<dd><code>--iter <var>num</var></code> : The number of iterations. (default: 10000)</dd>
<dd><code>--size <var>num</var></code> : The size of each record value. (default: 8)</dd>
<dd><code>--threads <var>num</var></code> : The number of threads. (default: 1)</dd>
<dd><code>--verbose</code> : Prints verbose reports.</dd>
<dd><code>--path <var>path</var></code> : The path of the file to write or read.</dd>
<dd><code>--file <var>impl</var></code> : The name of a file implementation: mmap-para, mmap-atom, pos-para, pos-atom. (default: mmap-para)</dd>
<dd><code>--no_wait</code> : Fails if the file is locked by another process.</dd>
<dd><code>--no_lock</code> : Omits file locking.</dd>
<dd><code>--alloc_init <var>num</var></code> : The initial allocation size. (default: 1048576)</dd>
<dd><code>--alloc_inc <var>num</var></code> : The allocation increment factor. (default: 2.0)</dd>
<dt>Options for the sequence subcommand:</dt>
<dd><code>--random_key</code> : Uses random keys rather than sequential ones.</dd>
<dd><code>--random_value</code> : Uses random length values rather than fixed ones.</dd>
<dd><code>--set_only</code> : Does only setting.</dd>
<dd><code>--get_only</code> : Does only getting.</dd>
<dd><code>--remove_only</code> : Does only removing.</dd>
<dt>Options for the parallel subcommand:</dt>
<dd><code>--random_key</code> : Uses random keys rather than sequential ones.</dd>
<dd><code>--random_value</code> : Uses random length values rather than fixed ones.</dd>
<dd><code>--keys</code> : The number of unique keys.</dd>
<dd><code>--rebuild</code> : Rebuilds the database occasionally.</dd>
<dd><code>--sleep <var>num</var></code> : The duration to sleep between iterations. (default: 0)</dd>
<dt>Options for the wicked subcommand:</dt>
<dd><code>--iterator</code> : Uses iterators occasionally.</dd>
<dd><code>--clear</code> : Clears the database occasionally.</dd>
<dd><code>--rebuild</code> : Rebuilds the database occasionally.</dd>
<dt>Options for the index subcommand:</dt>
<dd><code>--type <var>expr</var></code> : The types of the key and value of the index: file, mem, n2n, n2s, s2n, s2s, str. (default: file)</dd>
<dd><code>--random_key</code> : Uses random keys rather than sequential ones.</dd>
<dd><code>--random_value</code> : Uses random length values rather than fixed ones.</dd>
<dt>Options for HashDBM:</dt>
<dd><code>--append</code> : Uses appending rather than pre-defined ones.</dd>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4)</dd>
<dd><code>--align_pow <var>num</var></code> : Sets the power to align records. (default: 3)</dd>
<dd><code>--buckets <var>num</var></code> : Sets the number of buckets for hashing. (default: 1048583)</dd>
<dd><code>--fbp_cap <var>num</var></code> : Sets the capacity of the free block pool. (default: 2048)</dd>
<dd><code>--lock_mem_buckets</code> : Locks the memory for the hash buckets.</dd>
<dt>Options for TreeDBM:</dt>
<dd><code>--append</code> : Uses the appending mode rather than the in-place mode.</dd>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4)</dd>
<dd><code>--align_pow <var>num</var></code> : Sets the power to align records. (default: 10)</dd>
<dd><code>--buckets <var>num</var></code> : Sets the number of buckets for hashing. (default: 131101)</dd>
<dd><code>--fbp_cap <var>num</var></code> : Sets the capacity of the free block pool. (default: 2048)</dd>
<dd><code>--lock_mem_buckets</code> : Locks the memory for the hash buckets.</dd>
<dd><code>--max_page_size <var>num</var></code> : Sets the maximum size of a page. (default: 8130)</dd>
<dd><code>--max_branches <var>num</var></code> : Sets the maximum number of branches of inner nodes. (default: 256)</dd>
<dd><code>--max_chached_pages <var>num</var></code> : Sets the maximum number of cached pages. (default: 10000)</dd>
<dt>Options for SkipDBM:</dt>
<dd><code>--offset_width <var>num</var></code> : The width to represent the offset of records. (default: 4)</dd>
<dd><code>--step_unit <var>num</var></code> : Sets the step unit of the skip list. (default: 4)</dd>
<dd><code>--max_level <var>num</var></code> : Sets the maximum level of the skip list. (default: 14)</dd>
<dd><code>--sort_mem_size <var>num</var></code> : Sets the memory size used for sorting. (default: 268435456)</dd>
<dd><code>--insert_in_order</code> : Inserts records in ascending order order of the key.</dd>
<dd><code>--max_cached_records <var>num</var></code> : Sets the number of cached records (default: 65536)</dd>
<dd><code>--reducer <var>func</var></code> : Sets the reducer: none, first, second, last, concat, total. (default: none)</dd>
<dt>Options for TinyDBM and StdHashDBM:</dt>
<dd><code>--buckets <var>num</var></code> : Sets the number of buckets for hashing. (default: 1048583)</dd>
<dt>Options for CacheDBM:</dt>
<dd><code>--cap_rec_num <var>num</var></code> : Sets the maximum number of records. (default: 1048576)</dd>
<dd><code>--cap_mem_size <var>num</var></code> : Sets the total memory size to use.</dd>
<dt>Options for PolyDBM and ShardDBM:</dt>
<dd><code>--params <var>str</var></code> : Sets the parameters in "key=value,key=value" format.</dd>
</dl>

<p>The following is a sample usage to make a hash database file and then store 1 million records of 8-byte keys and 8-byte values, retrieve all of them, and remove all of them in sequence.  The number of threads is 10 and each thread handles 100 thousand records.  The number of buckets is 2 million.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_perf sequence --iter 100k --threads 10 \
  --dbm hash --path casket.tkh --buckets 2m
]]></code></pre>

<p>The following is a sample usage to make a skip database file and then store 1 million records of 8-byte keys and 8-byte values, retrieve all of them.  The number of threads is 10 and each thread handles 100 thousand records.  The step unit and the maximum level of the skip list are 3 and 13 respectively.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_perf sequence --iter 100k --threads 10 \
  --dbm skip --path casket.tks --step_unit 3 --max_level 13
]]></code></pre>

<h2 id="tips">Tips</h2>

<h3 id="tips_hashdbm_tune">Tuning HashDBM</h3>

<p>The file hash database uses a static hash table, whose size doesn't change implicitly.  The number of hash buckets is set when the database is created.  The load factor, which is defined as the ratio of the number of records to the number of buckets, should be less than 1, in order to maintain a good performance.  So, the number of buckets should be larger than the estimated number of records.  By default, the number of buckets is about 1 million.</p>

<p>Locking the memory for the hash buckets with the "lock_mem_buckets" parameter is also effective to get stable performance.  It uses the "mlock" system call to force the memory region for the hash buckets to be on RAM, not to be in the swap space.  The downside is that if you make a huge hash table or make many database files, out-of-memory errors can occur.</p>

<p>To create a very small database, you'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --dbm hash \
  --offset_width 3 --align_pow 0 --buckets 10000 casket.tkh
]]></code></pre>

<p>The same thing can be done in C++.</p>

<pre><code class="language-cpp"><![CDATA[HashDBM::TuningParameters tuning_params;
tuning_params.offset_width = 3;  // Up to 2^(8*3) = 16MB
tuning_params.align_pow = 0;     // No alignment
tuning_params.num_buckets = 10000;
dbm.OpenAdvanced("casket.tkh", true, File::OPEN_DEFAULT, tuning_params);
]]></code></pre>

<p>To make a very large database, you'll set offset_width=5, align_pow=4, and num_buckets=100000000 or something, and lock_mem_bucket.</p>

<p>When the load factor exceeds 1 and performance decreases, you'll rebuild the database.  By default, the number of buckets is set as twice as the number of records so that the load factor becomes 0.5.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util rebuild casket.tkh
]]></code></pre>

<p>The same thing can be done in C++.  The RebuildAdvanced method can take tuning parameters so that you can specifiy them explicitly.</p>

<pre><code class="language-cpp"><![CDATA[dbm.Rebuild();
]]></code></pre>

<p>One of the most important features of Tkrzw's file hash database is that the operation to rebuild the database doesn't block other threads trying to update the database.  Rebuilding is done by making a temporary database file while accepting updates to the original file.  When the rebuilding is done, updates during the rebuilding are applied to the new file incrementally and then the original file is replaced with the new file atomically.  All these operations are done just by calling the Rebuild method by a dedicated thread.</p>

<p>Rebuilding the database is also effective to resolve data fragmentation.  The file hash database has a metadata property of effective data size, which is the total size of keys and values of all records.  When the effective data size is less than 30% of the file size, it's time to rebuild the database.  To know whether the database should be rebuilt, you'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util inspect casket.tkh
Inspection:
  eff_data_size=10654160
  ...
File Size: 117952032
Number of Records: 665885
Should be Rebuilt: true
]]></code></pre>

<p>The same thing can be done in C++.</p>

<pre><code class="language-cpp"><![CDATA[bool to_be_rebuilt = false;
dbm.ShouldBeRebuilt(&to_be_rebult);
std::cout << to_be_rebult << std::endl;
std::cout << dbm.GetEffectiveDataSize() << std::endl; 
]]></code></pre>

<p>Alignment is effective to mitigate fragmentation when the size of the value of records are changed frequently.  By default, the alignemnt is set 2^3 = 8 bytes so that every records begins at an offset which is a multiple of 8.  If the record size is not a multiple of 8, padding bytes are put at the end.  If the size of a record increases but the padding can contain the increment, the record don't have to be moved to another place.  If the record is moved, the original position region becomes a free block.  In the in-place update mode, the free block can be reused but it is not assured.  Having 8-byte alignment means that the expected size of padding is 4 bytes.  If the typical record size is 30 bytes or something, 4 bytes padding seems acceptable and effective.</p>

<p>The position of each record is represented by a 4-byte integer by default.  It means that the maximum value is 4^32 = 4GiB.  Actually, the position is calculated as the value multiplied by the alignment.  So, the muximum size is 4GiB * 8 = 32GiB by default.  If the database size is about to exceed the limit, you should rebuild the database with an increased offset width.  Usually, 5-byte integer will do because it can represent up to 1TiB.  Usually, alignment is not used for the appending mode.  And, databases in the appending mode tend to be very large.  Therefore, you'll create a database in the appending mode like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --offset_width 5 --align_pow 0 --buckets 10m --append casket.tkh
]]></code></pre>

<h3 id="tips_hashdbm_update_modes">Update Modes of HashDBM</h3>

<p>The update mode is also set when creating or rebuilding a database.  In the in-place mode, an existing record is updated in-place if possible.  Let's say, there's two records in the database, A (key="A", value="aaa") and B (key="B", value="bbb").  The record section in the database file is like this.  "xxx" means metadata and "..." means padding.</p>

<pre>[xxx:A:aaa.....][xxx:B:bbb.....]
</pre>

<p>Now, the value of A is updated into "aaaaaa".  The padding can contain the additional size so the record is not moved.</p>

<pre>[xxx:A:aaaaaa..][xxx:B:bbb.....]
</pre>

<p>The value of A is updated into "aaaaaaaaa".  The padding cannot contain the additional size so the record is moved.  The original section becomes a free block.</p>

<pre>[xxx:*:........][xxx:B:bbb.....][xxx:A:aaaaaaaaa.......]
</pre>

<p>The free block is registered in the free block pool so it is reused for a new record whose size can be contained in it.  Let's say, a new record C (key="C", value="cccccc") is added.</p>

<pre>[xxx:C:cccccc..][xxx:B:bbb.....][xxx:A:aaaaaaaaa.......]
</pre>

<p>If a record is removed, the region for the record becomes a free block and it is reused later.  Therefore, in the in-place mode, the size of the database doesn't increase rapidly if the existing record is updated in a natural way.  Even so, fragmentation happens gradually so you should rebuild the database occasionally.</p>

<p>Meanwhile, in the appending mode, the existing region is never rewritten.  Let's say, the value of record A is updated from "aaa" to "aaaaaa".  A new record region is appended at the end of the file.</p>

<pre>[xxx:A:aaa][xxx:B:bbb][xxx:A:aaaaaa]
</pre>

<p>If A is removed, a new record region to mark it as removed is appended.</p>

<pre>[xxx:A:aaa][xxx:B:bbb][xxx:A:aaaaaa][xxx:A:(removed)]
</pre>

<p>All updating operations are linked from the end to the beginning.  And the hash table refers to the last operations.  Therefore, the latest state is always read.  Thanks to this structure, the existing information never breaks.  However, the size of the database file increases rapidly with any kind of updating operations.  So, you should rebuild the database frequently.</p>

<p>You might think that the in-place mode is faster and that the appending mode is slower.  However, it is not the case necessarily.  In the in-place mode, updated "dirty" pages of the I/O buffer scatter across the database file so that effectiveness of synchronizing the dirty pages with the device is low.  Meanwhile, in the appending mode, dirty pages occurs only at end of the file so that effectiveness of synchronizing the dirty pages with the device is high.  In other words, a database in the appending mode causes inefficient reading with random access but it causes efficient writing with sequential access.  Therefore, if rebuilding the database is done at a reasonable freqeuncy, the appending mode can be faster than the in-place mode.  SSD devices are good at random reading but not good at random writing.  Especially if you build a huge database on an SSD storage, the appending mode is worth to consider.</p>

<p>Let's build a hash database and store 100 million records (10 million for each of 10 threads).  And then retrieve and remove all of them.  First, we check performance of the in-place mode.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_perf sequence --iter 10m --threads 10 \
  --dbm hash --path casket-inplace.tkh --buckets 200m
]]></code></pre>

<p>On my environment, setting operations took 48 seconds, which meant 2.0 million QPS.  Getting operations took 25 seconds, which meant 3.9 million QPS.  And, removing operations took 83 seconds, which meant 1.2 million QPS.  The file size was 2.9GiB.  Removing operations took longer time than setting operations because removing existing records requires random access writing to reorganize bucket chains.  Then, let's do the same thing in the appending mode.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_perf sequence --iter 10m --threads 10 \
  --dbm hash --path casket-append.tkh --buckets 200m --append
]]></code></pre>

<p>This time, setting operations took 44 seconds, which meant 2.2 million QPS.  Getting operations took 26 seconds, which meant 3.8 million QPS.  And, removing operations took 54 seconds, which meant 1.8 million QPS.  The file size was 4.5GiB.  Throughput of setting and getting operations in this setting should be the same between the in-place mode and the appending mode.  However, removing operations in the appending mode should be faster than the in-place mode because appending causes only sequential writing.</p>

<h3 id="tips_treedbm_tune">Tuning TreeDBM</h3>

<p>The file tree database uses B+ tree.  Records are organized in ascending order of the key.  Records at adjacent positions are chunked as a unit called "page".  The larger each page is, the better space efficiency is.  And, performance of sequential access is also better.  Meanwhile, performance of random access deteriorates if pages are too big.  The default maximum page size 8130 is suitable in most cases.</p>

<p>If the machine has abundant memory, increasing the max_cached_pages parameter contributes better performance of random access.  By default, 10000 pages are cached.  The average page size is about 8K bytes so 80MB memory is used for the cache system.  Let's say, the machine has 2GB free memory.  Then, specifying 20,0000 or something is feasible.</p>

<p>Usually, there's no need to modify the default tuning parameters.  By default, the maximum database size is 1TB because the offset width is 4 and the alignment power is 8.  If the number of records exceeds 20 million, the num_buckets parameter should be increased for better performance.  Locking the memory for the hash buckets with the "lock_mem_buckets" parameter is also effective to get stable performance.</p>

<p>To create a very small database, you'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --dbm tree \
  --offset_width 3 --align_pow 0 --max_page_size 2000 --max_branches 128
]]></code></pre>

<p>The same thing can be done in C++.</p>

<pre><code class="language-cpp"><![CDATA[TreeDBM::TuningParameters tuning_params;
tuning_params.offset_width = 3;  // Up to 2^(8*3) = 16MB
tuning_params.align_pow = 0;     // No alignment.
tuning_params.max_page_size = 2000;
dbm.OpenAdvanced("casket.tkt", true, File::OPEN_DEFAULT, tuning_params);
]]></code></pre>

<p>To make a very large database which are accessed at random, you'll set align_pow=12 and max_page_size=4080.  Due to the alignment power, every hash record is aligned at a multiple of 4096 bytes, which maximize efficiency of the underlying I/O system.  As the maximum page size doesn't include footprint of the hash record of 16 bytes, 4096 - 16 = 4080 is the ideal size.  Note that 4080 is the maximum size so the actual usage is less than it.  However, having padding space is beneficial to mitigate movement of page positions.  Increasing the max_cached_pages is also better as long as the memory capacity allows for it.</p>

<p>Let's say, there is a leaf page containing only one small record.  If the alignment is 4096 bytes, most space is filled with padding bytes.</p>

<pre>[XXX:A:aaa............................]
</pre>

<p>Some records are inserted into the leaf page.  If the total page size doesn't exceeds the allocated size, the page doesn't have to reallocated.</p>

<pre>[XXX:A:aaa:B:bbb:C:ccc:D:ddd..........]
</pre>

<p>If a new record is added to the leaf page and the total page size exceeds the allocated size, the page is reallocated and the original position becomes a free space.</p>

<pre>[XXX:*********************************][XXX:A:aaa:B:bbb:C:ccc:D:ddd:E:eee:F:fff:G:ggg:H:hhh:I:iii...]
</pre>

<p>However, if the maximum page size setting is less than the alignment size, the page is divided and it avoids reallocation.  Half of records are moved to a new page.  Therefore, having a large alignment and setting the maximim page size a bit less than the alignemnt pay in the long run.</p>

<pre>[XXX:A:aaa:B:bbb:C:ccc:D:ddd:E:eee:...][XXX:F:fff:G:ggg:H:hhh:I:iii..........]
</pre>

<p>In case that the cache cannot contain all records, however you tune the tree database, performance of random access cannot be comparable to the file hash database.  Thus, if you don't need ordered record access, using the file hash database is recommended.  If you need ordered record access and updating is done at random, consider using the file skip database, which is more scalable.</p>

<h3 id="tips_treedbm_comparators">Comparators of TreeDBM</h3>

<p>By default, the file tree database uses a lexical comparator which compares keys in lexicographical order.  This is suitable for many use cases.  If you want to use numeric values, representing anumber as a big-endian binary is a good idea because the default comparator sorts it properly.  However, if you want, the following built-in comparator can be specified via the tuning parameters.</p>

<ul>
<li><code>LexicalKeyComparator</code> : Compares keys as strings in lexicographical order.</li>
<li><code>LexicalCaseKeyComparator</code> : Compares keys as strings but ignore cases in ASCII code.</li>
<li><code>DecimalKeyComparator</code> : Compares keys as decimal integers like "1" and "99".</li>
<li><code>HexadecimalKeyComparator</code> : Compares keys as hexadecimal integers like "8A" and "FF3C".</li>
<li><code>RealNumberKeyComparator</code> : Compares keys as decimal real numbers like "2.1" and "-5.82".</li>
</ul>

<p>You can implement your own comparator.  In that case, you must specify it every time when you open the database.</p>

<p>By the way, databases other than the file skip database don't support duplicated keys.  That's because duplicated keys deteriorates time and space efficiency.  If you have to handle duplicated keys, build a skip database first and merge duplicated records.  Then, you can convert it to another type of database.</p>

<h3 id="tips_skipdbm_tune">Tuning SkipDBM</h3>

<p>The file skip database uses a skip list based on sorted records.  Whereas its time efficiency is worse than that of the hash table, its space efficiency is better.  Typically, the footprint for each record is less than 5 bytes.  Usually, there's no need to modify the default tuning parameters.  If the size of the database can exceed 4GB, the offset width parameter should be set 5.  If the number of records is more than 1 billion, the maximum level parameter should be set 15 or more.  If the main usage of the database is for sequential access, the step unit can be 16 and the maximum level can be 8.</p>

<p>To create a very small database, you'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --dbm skip \
  --offset_width 3 --step_unit 4 --max_level 8 casket.tkh
]]></code></pre>

<p>The same thing can be done in C++.</p>

<pre><code class="language-cpp"><![CDATA[SkipDBM::TuningParameters tuning_params;
tuning_params.offset_width = 3;  // Up to 2^(8*3) = 16MB
tuning_params.step_unit = 4;     // Minimum skip is 4 records
tuning_params.max_level = 8;     // Can skip 4^8 = 65536 records
dbm.OpenAdvanced("casket.tks", true, File::OPEN_DEFAULT, tuning_params);
]]></code></pre>

<p>To make a very large database, you'll set offset_width=5, step_unit=8, max_level=12, and sort_mem_size=2GB.  Increasing the step_unit parameter reduces frequency of random access.  Increasing the sort_mem_size parameter is a good idea because the default value is 256MB.</p>

<p>If you want to optimize the time/space efficiency by all means, rebuilding the database might help because optimal parameters are set implicitly.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util rebuild --dbm skip casket.tks
]]></code></pre>

<p>The same thing can be done in C++.  The RebuildAdvanced method can take tuning parameters so that you can specify them explicitly.</p>

<pre><code class="language-cpp"><![CDATA[dbm.Rebuild();
]]></code></pre>

<p>Rebuilding the database blocks other threads completely.  In the first place, the file skip database is not suitable for random updates.  Use the file hash database for that purpose.  The skip list database is suitable for batch processing.</p>

<p>Although the file skip database uses much resources to build the database file, space efficiency of the database file is the best.  Therefore, it is practical to build a large database on a high-performance machine and distribute it to low-performance machines where data retrieval is done.</p>

<p>You can improve performance of data retrieval on the file skip database by increasing the maximum number of cached records.  By default, 65536 records are cached.  Because nodes whose indices are multiples of the step unit or its powered numbers.  You can set the parameter like this.</p>

<pre><code class="language-cpp"><![CDATA[SkipDBM::TuningParameters tuning_params;
tuning_params.max_cached_records = 200000;
dbm.OpenAdvanced("casket.tks", false, File::OPEN_DEFAULT, tuning_params);
]]></code></pre>

<h3 id="tips_skipdbm_building">Building SkipDBM</h3>

<p>The skip database is composed of records sorted by the key.  Although you can insert records randomly, they are not visible until you call the Synchornize method, which sorts records implicitly and merge them with existing records of the database.  In other words, updating the skip database is done in an offline (batch) manner, not in an online manner.  If you already have records which are sorted in ascending order of the key, you can use the insert_in_order mode, which is very quick and scalable.  You can input multiple records of the same key and the order of insertion within records of the same key is preserved in the database.  Note that the order of records of different keys must strictly be consistent to std::less&lt;std::string&gt; if you use the insert_in_order mode.  In contrast, if your records are not sorted, you use the default mode, which sorts the records implicitly with merge sort on temporary files.  The reason of using temporary files is to build a huge database exceeding the memory capacity.  You can input multiple records with the same key here too.  The order within records of the same key is preserved during merge sort because it is a stable sort.</p>

<p>You can specify an arbitrary reducer to handle records of the same key.  By default, no reducer is applied and all records are passed through.  Whereas the following built-in reducers are bundled, you can implement your own reducers too.</p>

<ul>
<li><code>ReduceToFirst</code> : Keeps the first record.</li>
<li><code>ReduceToSecond</code> : Keeps the second record if it exists.  Otherwise, keeps the first record.</li>
<li><code>ReduceToLast</code> : Keeps the last record.</li>
<li><code>ReduceConcat</code> : Concatenates all values and makes a single record.</li>
<li><code>ReduceConcatWithNull</code> : Concatenates all values and makes a single record.  Null code is used as separators.</li>
<li><code>ReduceConcatWithTab</code> : Concatenates all values and makes a single record.  Tab is used as separators.</li>
<li><code>ReduceConcatWithLine</code> : Concatenates all values and makes a single record.  Linefeed is used as separators.</li>
<li><code>ReduceToTotal</code> : Totals the numeric values and makes a single record.</li>
</ul>

<p>If the input records have multiple records of the same key and you want to keep the first one, you'll finish the database with ReduceToFirst.  The following code keeps records of {"foo":"first"} and {"bar":"primero"} only.</p>

<pre><code class="language-cpp"><![CDATA[dbm.Set("foo", "first");
dbm.Set("foo", "second");
dbm.Set("foo", "third");
dbm.Set("bar", "primero");
dbm.SynchronizeAdvanced(false, nullptr, SkipDBM::ReduceToFirst);
]]></code></pre>

<p>In principle, the file skip database should be considered as a "static" database which are not updated after building it.  However, you can modify it with usual methods of DBM.  You can insert arbitrary records, which are put after existing records of the same keys.  Then, you can reduce them when calling the Synchronize method.  If you want to keep the latest value to overwrite the old value, you'll synchronize the database with ReduceToLast.  Assuming we use the above finished database, the following code keeps records of {"foo":"fifth"} and {"bar":"segundo"} only.</p>

<pre><code class="language-cpp"><![CDATA[dbm.Unfinish();
dbm.Set("foo", "fourth");
dbm.Set("foo", "fifth");
dbm.Set("bar", "segundo");
dbm.SynchronizeAdvanced(false, nullptr, SkipDBM::ReduceToLast);
]]></code></pre>

<p>You can insert new data, but how do you remove the existing data?  It is implemented as insertion of a special value.  If you call the Remove method, a special value called REMOVING_VALUE is inserted.  When synchronizing the database, if the special value is detected, records of the same key before the special value are removed.  Then, the reducer is applied if any.  The following code removes records of "foo" but doesn't remove records of "bar".</p>

<pre><code class="language-cpp"><![CDATA[dbm.Unfinish();
dbm.Set("foo", "fourth");
dbm.Remove("foo");
dbm.Remove("bar");
dbm.Set("bar", "tercero");
dbm.SynchronizeAdvanced(false, nullptr, SkipDBM::ReduceToLast);
]]></code></pre>

<p>Note that synchronizing the database requires rebuilding the entire database even if only one record is modified.  Therefore, updating records should be done in a batch processing style, where you do every operation and then synchronize the database.  Note again that you cannot access new records until you call the Synchronize method.</p>

<p>You can merge multiple skip database files after building them separately.  You can apply a reducer there.  If you have to aggregate a large amount of data, building partial results as separate database files and merge them afterward is a practical way.  You can do the same thing in C++ by calling the MergeSkipDatabase method.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util merge --reducer last \
  merged.tks source1.tks source2.tks source3.tks
]]></code></pre>

<p>The "merge" subcommand of tkrzw_dbm_util is useful to apply a reducer even if you specify no source databases to merge.</p>

<h3 id="tips_skipdbm_mapreduce">SkipDBM as a MapReduce</h3>

<p>You can use SkipDBM as a framework of MapReduce.  Map means an operation to generate and classify records from the given data source.  Reduce means an operation to process records for each class and store the result.  Between Map and Reduce, the framework does merge sort to organize records for each class.  Reading data source and adding records to the database are considered a Map operation.  And, the reducer applied when finishing the database represents a Reduce operation.  The following code implements word counting.</p>

<pre><code class="language-cpp"><![CDATA[SkipDBM text_dbm;
text_dbm.Open("text.tks", false);
SkipDBM count_dbm;
count_dbm.Open("word_count.tks", true);

auto iter = text.MakeIterator();
iter->First();
std::string doc_id, text;
while (iter->Get(&doc_id, &text) == Status::SUCCESS) {
  const std::vector<std::string> words = GetWords(text);
  for (const auto& word : words) {
    count_dbm(word, "1");
  }
  iter->Next();
}
count_dbm.SynchronizeAdvanced(false, nullptr, SkipDBM::ReduceToTotal);

count_dbm.Close();
text_dbm.Close();
]]></code></pre>

<h3 id="tips_memdbm_tune">Tuning TinyDBM, BabyDBM, and CacheDBM</h3>

<p>On-memory databases are convenient to manage large amount of objects while saving memory as much as possible.  Objects to be stored must be serialized as strings.  Thus, choosing an efficient serialization format is important.  Of all on-memory databaess, TinyDBM is the best in time efficiency.  BabyDBM is the best in space efficiency.  It also supports ordered access of records.  CacheDBM is suitable for handling cache data where old records are discarded implicitly.</p>

<p>TinyDBM has a tuning parameter for the number of buckets.  The number of buckets should be the same as or more than the number of stored records.  It is set with the constructor.</p>

<p>BabyDBM has a tuning parameter for the comparison function of keys.  The default comparison function is a lexical comparator, which is also usable for integers serialized as big-endian binary strings.  It is set with the constructor.</p>

<p>CacheDBM has two tuning parameters for the capaicty: the maximum number of records and the maximum memory usage.  The maximum number of records is necessary because it is used to determine the number of buckets.  Therefore, setting too large value is not good for space efficiency.  The maximum memory usage is used as an insurance to ensure that memory usage doesn't exceeds the specified value.</p>

<p>If you use integers as keys and/or values, using functions IntToStrBigEndian and StrToIntBigEndian as a serializer and a deserializer is a good idea for space efficiency.  This technique is usable for file databases too.</p>

<pre><code class="language-cpp"><![CDATA[BabyDBM dbm;
const std::string key = IntToStrBigEndian(1234);
const std::string value = IntToStrBigEndian(56789);
dbm.Set(key, value);
std::string got_value;
if (dbm.Get(key, &value).IsOK()) {
  std::cout << StrToIntBigEndian(got_value) << std::endl;
}
]]></code></pre>

<p>The DBM interface provides the Append method which appends the given value at the end of the value of the existing record.  Most DBM implementations use a wrapper which calls the Get method and the Set method atomically.  However, if you call the Append method N times, the time complexity is O(N^2).  To overcome the problem, TinyDBM and BabyDBM override it with specialized implementations, which write the given value directly at the end of the existing value if possible.  The amortized time complexity of the specialized implemenations is O(N).  Therefore, TinyDBM and BabyDBM are useful as buffers to build inverted indices for full-text search systems.</p>

<h3 id="tips_index_performance">Performance of ShardDBM</h3>

<p>You can shard a database into several database files by using ShardDBM.  Because the collision rate of locking mutex is divided by the number of shards, concurrency performance improves.  To confirm it, let's do performance tests.  This time we store 10 million records with 10 threads.  The key is an 8-byte string from "00000000", "00000001", to "09999999" selected at random.  We use ShardDBM and the internal database class varies for each test run.  First, let's see the result with one shard.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">2,281,733 QPS</td>
<td class="num">3,711,545 QPS</td>
<td class="num">2,577,823 QPS</td>
<td class="num">181.0 MB</td>
<td class="num">182.8 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">715,372 QPS</td>
<td class="num">2,398,091 QPS</td>
<td class="num">625,034 QPS</td>
<td class="num">385.7 MB</td>
<td class="num">119.8 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">510,634 QPS</td>
<td class="num">866,662 QPS</td>
<td class="num">839,042 QPS</td>
<td class="num">420.5 MB</td>
<td class="num">122.5 MB</td>
</tr>
</table>

<p>Then, let's see the result of ten shards.  Tuning parameters such as the number of backets and the maximum number of cached pages are specified to see optimal performance for all test runs.</p>

<table>
<tr>
<th>class</th>
<th>Set</th>
<th>Get</th>
<th>Remove</th>
<th>memory usage</th>
<th>file size</th>
</tr>
<tr>
<td>HashDBM</td>
<td class="num">5,163,622 QPS</td>
<td class="num">6,321,919 QPS</td>
<td class="num">5,653,435 QPS</td>
<td class="num">181.3 MB</td>
<td class="num">182.8 MB</td>
</tr>
<tr>
<td>TreeDBM</td>
<td class="num">2,373,253 QPS</td>
<td class="num">3,739,910 QPS</td>
<td class="num">2,263,250 QPS</td>
<td class="num">387.5 MB</td>
<td class="num">119.9 MB</td>
</tr>
<tr>
<td>SkipDBM</td>
<td class="num">564,395 QPS</td>
<td class="num">1,712,049 QPS</td>
<td class="num">1,220,438 QPS</td>
<td class="num">387.8 MB</td>
<td class="num">122.5 MB</td>
</tr>
</table>

<p>As seen above, sharding is effective to improve throughput with multi-threading.  Especially, if updating operations are the bottleneck of the service, sharding is beneficial.  Alghouth there's a drawback that the iterator can be slower, the extent is usually acceptable.  The following commands reproduce the performance tests on your environment.</p>

<pre><code class="language-shell-session"><![CDATA[# Random tests by ten threads and one shard.
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tkh \
  --params "num_shards=1,dbm=hash,num_buckets=10000000"
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tkt \
  --params "num_shards=1,dbm=tree,num_buckets=100000,max_cached_pages=100000"
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tks \
  --params "num_shards=1,dbm=skip" --reducer last

# Random tests by ten threads and ten shard.
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tkh \
  --params "num_shards=10,dbm=hash,num_buckets=1000000"
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tkt \
  --params "num_shards=10,dbm=tree,num_buckets=10000,max_cached_pages=10000"
$ tkrzw_dbm_perf sequence --iter 1000000 --threads 10 --random_key --dbm shard --path casket.tks \
  --params "num_shards=10,dbm=skip" --reducer last
]]></code></pre>

<h3 id="tips_index_performance">Performance of Secondary Indices</h3>

<p>If you use secondary indices, choosing suitable classes is important.  FileIndex is relatively slow but the data is perpetuated in a file.  MemIndex is fast and memory efficient.  StdIndex is a template class.  Time efficiency and space efficiency differ by the types of the key and the value.  Let's say, 10 threads set 1 million records in total.  The key and the value of each record are numeric values.  We compare int64_t (from 0 to 999999) and decimal std::string (from "0" to "999999").  In addition, we check MemIndexStr which is a specialized version for memory efficiency of string-to-string use cases.</p>

<table>
<tr>
<th>class</th>
<th>Add</th>
<th>Check</th>
<th>Remove</th>
<th>Memory Usage</th>
</tr>
<tr>
<td>FileIndex</td>
<td class="num">700,275 QPS</td>
<td class="num">2,451,437 QPS</td>
<td class="num">750,214 QPS</td>
<td class="num">21.9 MB</td>
</tr>
<tr>
<td>MemIndex</td>
<td class="num">1,031,907 QPS</td>
<td class="num">5,703,882 QPS</td>
<td class="num">1,003,391 QPS</td>
<td class="num">50.0 MB</td>
</tr>
<tr>
<td>StdIndex&lt;int64_t, int64_t&gt;</td>
<td class="num">1,228,569 QPS</td>
<td class="num">11,051,806 QPS</td>
<td class="num">1,739,578 QPS</td>
<td class="num">62.0 MB</td>
</tr>
<tr>
<td>StdIndex&lt;int64_t, string&gt;</td>
<td class="num">944,862 QPS</td>
<td class="num">9,911,980 QPS</td>
<td class="num">1,309,429 QPS</td>
<td class="num">76.9 MB</td>
</tr>
<tr>
<td>StdIndex&lt;string, int64_t&gt;</td>
<td class="num">806,142 QPS</td>
<td class="num">8,744,327 QPS</td>
<td class="num">1,040,206 QPS</td>
<td class="num">76.8 MB</td>
</tr>
<tr>
<td>StdIndex&lt;string, string&gt;</td>
<td class="num">580,958 QPS</td>
<td class="num">7,972,128 QPS</td>
<td class="num">862,372 QPS</td>
<td class="num">106.7 MB</td>
</tr>
<tr>
<td>StdIndexStr</td>
<td class="num">397,067 QPS</td>
<td class="num">6,731,285 QPS</td>
<td class="num">700,590 QPS</td>
<td class="num">76.8 MB</td>
</tr>
</table>

<p>Using StdIndex&lt;int64_t, int64_t&gt; as much as possible is good for both time and space efficiency.  Otherwise, using MemIndex is the second best.  If you need file indices, using FileIndex is the sole solution.</p>

<p>The above test cases should show ideal performance of each index class because records are accessed sequentially and the entire data can be cached on memory.  Then, let's move on to tougher test cases.  We build a large database of 100 million records with random keys between "00000000" and "99999999".  As some keys are duplicated and such records are overwritten, the actual number of unique records is about 63 million.  We use max_cached_pages=400000 for FileIndex.</p>

<table>
<tr>
<th>class</th>
<th>Add</th>
<th>Check</th>
<th>Remove</th>
<th>Memory Usage</th>
</tr>
<tr>
<td>FileIndex</td>
<td class="num">61,779 QPS</td>
<td class="num">216,758 QPS</td>
<td class="num">63,744 QPS</td>
<td class="num">7202.7 MB</td>
</tr>
<tr>
<td>MemIndex</td>
<td class="num">455,743 QPS</td>
<td class="num">2,428,402 QPS</td>
<td class="num">392,666 QPS</td>
<td class="num">6144.4 MB</td>
</tr>
<tr>
<td>StdIndex&lt;int64_t, int64_t&gt;</td>
<td class="num">158,152 QPS</td>
<td class="num">4,935,703 QPS</td>
<td class="num">142,164 QPS</td>
<td class="num">5962.8 MB</td>
</tr>
<tr>
<td>StdIndex&lt;int64_t, string&gt;</td>
<td class="num">128,086 QPS</td>
<td class="num">4,528,615 QPS</td>
<td class="num">128,206 QPS</td>
<td class="num">7453.0 MB</td>
</tr>
<tr>
<td>StdIndex&lt;string, int64_t&gt;</td>
<td class="num">114,269 QPS</td>
<td class="num">3,768,720 QPS</td>
<td class="num">110,755 QPS</td>
<td class="num">7452.9 MB</td>
</tr>
<tr>
<td>StdIndex&lt;string, string&gt;</td>
<td class="num">105,488 QPS</td>
<td class="num">3,477,022 QPS</td>
<td class="num">101,512 QPS</td>
<td class="num">10433.1 MB</td>
</tr>
<tr>
<td>StdIndexStr</td>
<td class="num">98,652 QPS</td>
<td class="num">2,798,903 QPS</td>
<td class="num">96,561 QPS</td>
<td class="num">10422.1 MB</td>
</tr>
</table>

<p>In this settings, using FileIndex slows down significantly.  If you need more throughput than 61K QPS, you should use on-memory indices.  If parallelism of updating operations is important, using MemIndex is the best.  Otherwise, using StdIndex is recommended.  Using integer type as much as possible is a good idea.</p>

<h3 id="tips_hashdbm_restore">Restoring Broken Databases</h3>

<p>An opened database should be closed properly.  Otherwise, the database file will be broken.  When a database is opened, a field in the metadata section is marked as "opened".  When the database is closed, the field is marked as "closed".  If the process opening a database crashes without closing the database, the field is still marked as "opened".  When the database file is opened the next time, the library detects the past incident and the database is considered unhealthy.  Unhealthy databases cannot be updated in order to protect remaining data.  You can check it by a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util inspect casket.tkh 
Inspection:
  healthy=false
  num_records=0
  file_size=8396800
  mod_time=1586714806497266
  ...
File Size: 8396800
Number of Records: 0
Should be Rebuilt: true
]]></code></pre>

<p>"num_records", "file_size", and "mod_time" represent information when the database was closed successfully at the last time.  So, even if you add many records before the last crash, the metadata doesn't reflect them.  However, written record data can be salvaged by restoring the database.  You'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util restore casket.tkh casket-restored.tkh
]]></code></pre>

<p>If you want to set tuning parameters to the new restored database, you can prepare the new database with the tuning parameters and set it as the destination.  This operation can be done even for a healthy database to rebuild it.</p>

<p>The same thing can be done in C++.  HashDBM, TreeDBM, and SkipDBM have the RestoreDatabase method.  PolyDBM and ShardDBM also privides wrapper functions.</p>

<pre><code class="language-cpp"><![CDATA[HashDBM::RestoreDatabase("casket.tkh", "casket-restored.tkh", -1);
]]></code></pre>

<p>To restore sharded database files like (casket.tkh-00000-of-00002 and casket.tkh-00001-of-00002), you'll run a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util restore --dbm shard --class hash casket.tkh casket-restored.tkh
]]></code></pre>

<p>Databases in the appending mode can utilize an interesting feature called snapshot reproduction.  As every updating operation is recorded and serialized in the database file, you can reproduce them until a certain moment.  And, the moment is represented as the then size of the database file.  Given that you have the file size of the moment by calling the FileSize method, you can restore the database of the state at the moment.  You'll check it by commands like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --append casket.tkh  # Appending mode.
$ tkrzw_dbm_util set casket.tkh one first
$ tkrzw_dbm_util set casket.tkh two second
$ ls -l casket.tkh  # Get the size when there are two records.
-rw-r--r-- 1 mikio mikio 4198440 Feb 11 18:46 casket.tkh
$ tkrzw_dbm_util set casket.tkh three third
$ tkrzw_dbm_util list casket.tkh  # The original database has three.
three  third
two    second
one    first
$ tkrzw_dbm_util restore --end_offset 4198440 casket.tkh casket-restore.tkh
$ tkrzw_dbm_util list casket-restore.tkh  # The restored database has two.
two    second
one    first
]]></code></pre>

<p>As the file tree database is based on the file hash database, snapshot reproduction can be done for the file tree database.</p>

<h3 id="tips_hashdbm_backup">Making Backup Data Online</h3>

<p>Let's say, your service runs 24 hours a day and any downtime is not allowed.  Then, you have to make backup data of the database online without halting the service.  There are two methods to do so: CopyFile and Export.</p>

<p>The CopyFile method synchronizes the updated content to the database file and copy the whole data to another file.  As copying is done while the content of the database is synchronized and frozen, the copied file is considered as a snapshot of the database at a certain moment.  And, the copying operation is done by the fastest way on the system, like the sendfile system call.</p>

<pre><code class="language-cpp"><![CDATA[dbm.CopyFile("casket-backup-20200515.tkh");
]]></code></pre>

<p>The downside of the CopyFile method is that other threads trying to update the database are blocked during the copying operation, which can take several seconds or minutes depending on the file size.  If it is not acceptable, use the Export method, which accesses each and every records and adds them to another database.  As other threads updating the database are not blocked during the iteration, atomicity as the database is not assured.  Atomicity of each record is assured.</p>

<pre><code class="language-cpp"><![CDATA[HashDBM backup_dbm;
backup_dbm.Open("casket-backup-20200515.tkh", true);
dbm.Export(&backup_dbm);
backup_dbm.Close();
]]></code></pre>

<p>Some file systems provides a feature to make a snapshot of a file.  The Synchronize method can call a callback function while the database is synchronized.  Therefore, if the callback function uses the snapshot feature, you can make an atomic snapshot at a moment without making other threads wait.</p>

<pre><code class="language-cpp"><![CDATA[class Snapshooter : public DBM::FileProcessor {
 public:
  explicit Snapshooter(const std::string& snapshot_path) : snapshot_path_(snapshot_path) {}
  void Process(const std::string& path) override {
    SomehowMakeSnapShot(path, snapshot_path_);
  }
 private:
  std::string snapshot_path_;
} snapshooter("casket-backup-20200515.tkh");
dbm.Synchronize(false, &snapshooter);
]]></code></pre>

<h3 id="tips_migration">Data Migration</h3>

<p>A simple format called FlatRecord is provided as a vehicle of data migration.  Flat records are also used to save records of on-memory databases in a file.  A binary file of FlatRecord contains a sequence of records.  Each record represents a sequence of bytes, in the following format.</p>

<dl>
<dt>The magic data</dt>
<dd>1 byte integer.</dd>
<dt>The data size</dt>
<dd>A byte delta encoded integer.</dd>
<dt>The data</dt>
<dd>Arbitrary string data.</dd>
</dl>

<p>The magic data is 0xFF and helps us detect data corruption.  The data size is represented in byte delta encoding.  A value between 0 and 127 takes 1 byte.  A value between 128 and 16383 takes 2 bytes. A value between 16384 and 2097151 takes 3 bytes.  A key-value pair is represented by two records in sequence.  To represent a key-value database, keys and values of records appear alternately.</p>

<p>You can export all records of an existing database into a flat record file, by a command like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util export casket.tkh external.flat
]]></code></pre>

<p>If all keys and values are plain text, exporting them as TSV is a feasible option.  You can do it like this.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util export --tsv casket.tkh external.tsv
]]></code></pre>

<p>Records in exported data files can be imported into a database, by commands like these.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util import casket.tkh external.flat
$ tkrzw_dbm_util import --tsv casket.tkh external.tsv
]]></code></pre>

<p>Whereas data of flat records can include any kind of binary data or text data, TSV data cannot include tab, linefeed or binary data.  If you want to escape output data in C-style for exporting and unescape input data for importing, add the "--escape" flag.</p>

<h3 id="tips_util_funcs">Utility Functions</h3>

<p>Utility functions are provided to complement functionality of the DBM interface.  They are defined in "tkrzw_dbm_common_impl.h".  Especially, these are useful.</p>

<ul>
<li><code>SearchDBM</code> : Searches a database and get keys which match a pattern.</li>
<li><code>SearchDBMForwardMatch</code> : Searches a database and get keys which begin with a pattern.</li>
<li><code>SearchDBMRegex</code> : Searches a database and get keys which match a regular expression.</li>
<li><code>SearchDBMEditDistance</code> : Searches a database and get keys whose edit distance with a pattern is the least.</li>
</ul>

<p>Scanning the whole database is sometimes costly.  If the database is not updated frequently, exporting keys into a text file and scanning it is more efficient.  The ExportDBMKeysAsLines function is provided for the purpose.  And, the functions SearchTextFile, SearchTextFileRegex, and SearchTextFileEditDistance are available for such text files.</p>

<h3 id="tips_multitasking">Multitasking</h3>

<p>Databases of Tkrzw are designed to be shared among multiple threads.  However, they are not designed to be shared among multiple processes.  When one process opens a database file as a writer, an exclusive lock is applied to the file so that other processes trying to open the same database file are blocked until the database is closed.  When one process opens a database file as a reader, a shared lock is applied to the file so that other reader process can open the same database but other writer process are blocked until the database is closed.  Thereby, consistency of the state of the database and atomicity of updating operations are assured in the multiprocessing environment.</p>

<p>A workaround way to share the same database among multiple processes is that each operation to access the database is done between opening the database and closing the database.  Although it causes unnecessarily read/write operations for each database access, it is feasible up to a certain throughput (100 QPS or something).  The workaround is useful with traditional shell commands and CGI scripts.  If the throughput requirement or the latency requirement is more than a threshold, you should write a multithread application program to serve the database.</p>

<h3 id="tips_file_classes">File Classes</h3>

<p>The file databases are based on abstract file storage interface so that you can build a database on any kind of file systems.  Several concrete implementations on POSIX API are bundled.  MemoryMapParallelFile is used for the file hash database by default.</p>

<ul>
<li><b>MemoryMapParallelFile</b> : Based on memory mapping I/O by "mmap".  Access to the data is not guarded by mutex.</li>
<li><b>MemoryMapAtomicFile</b> : Based on memory mapping I/O by "mmap".  Access to the data is guarded by mutex and update looks atomic.</li>
<li><b>PositionalParallelFile</b> : Based on positional I/O by "pread" and "pwrite".  Access to the data is not guarded by mutex.</li>
<li><b>PositionalAtomicFile</b> : Based on positional I/O by "pread" and "pwrite".  Access to the data is guarded by mutex and update looks atomic.</li>
</ul>

<p>Consistency of the data as a database is guaranteed by the database classes.  So, atomicity of update is not required to the file class.  Therefore, there's no need to use MemoryMapAtomicFile and PositionalAtomicFile for purposes other than performance test.</p>

<p>Because the MemoryMapParallelFile uses memory mapping, efficiency of I/O is the best.  However, the file size cannot exceed the size of the virtual memory.  If the file size is larger, the PositionalParallelFile should be used.</p>

<p>To check performance, let's write 10 million records each of which is 100 bytes with one thread.  The file size is 954MB.</p>

<table>
<tr>
<th>class</th>
<th>Sequential Write</th>
<th>Sequential Read</th>
<th>Random Write</th>
<th>Random Read</th>
</tr>
<tr>
<td>MemoryMapParallelFile</td>
<td class="num">7,581,742 QPS</td>
<td class="num">12,014,696 QPS</td>
<td class="num">4,563,364 QPS</td>
<td class="num">4,838,227 QPS</td>
</tr>
<tr>
<td>MemoryMapAtomicFile</td>
<td class="num">7,858,269 QPS</td>
<td class="num">12,794,807 QPS</td>
<td class="num">4,480,116 QPS</td>
<td class="num">4,905,746 QPS</td>
</tr>
<tr>
<td>PositionalParallelFile</td>
<td class="num">950,962 QPS</td>
<td class="num">1,351,362 QPS</td>
<td class="num">739,976 QPS</td>
<td class="num">1,018,328 QPS</td>
</tr>
<tr>
<td>PositionalAtomicFile</td>
<td class="num">950,187 QPS</td>
<td class="num">1,331,649 QPS</td>
<td class="num">735,836 QPS</td>
<td class="num">993,415 QPS</td>
</tr>
</table>

<p>Do the same things with 10 threads.  Each thread write 1 million records and read them.</p>

<table>
<tr>
<th>class</th>
<th>Sequential Write</th>
<th>Sequential Read</th>
<th>Random Write</th>
<th>Random Read</th>
</tr>
<tr>
<td>MemoryMapParallelFile</td>
<td class="num">7,698,532 QPS</td>
<td class="num">10,020,994 QPS</td>
<td class="num">7,643,432 QPS</td>
<td class="num">9,144,095 QPS</td>
</tr>
<tr>
<td>MemoryMapAtomicFile</td>
<td class="num">2,378,891 QPS</td>
<td class="num">10,230,659 QPS</td>
<td class="num">1,427,267 QPS</td>
<td class="num">9,538,727 QPS</td>
</tr>
<tr>
<td>PositionalParallelFile</td>
<td class="num">1,084,185 QPS</td>
<td class="num">5,610,970 QPS</td>
<td class="num">770,063 QPS</td>
<td class="num">5,360,002 QPS</td>
</tr>
<tr>
<td>PositionalAtomicFile</td>
<td class="num">182,761 QPS</td>
<td class="num">5,434,638 QPS</td>
<td class="num">136,015 QPS</td>
<td class="num">5,011,813 QPS</td>
</tr>
</table>

<p>For the command line, the "--file" option must be set always if you use a file class except for MemoryMapParallelFile.</p>

<pre><code class="language-shell-session"><![CDATA[$ tkrzw_dbm_util create --file pos-para casket.tkh
$ tkrzw_dbm_util set --file pos-para casket.tkh Japan Tokyo
$ tkrzw_dbm_util get --file pos-para casket.tkh Japan
]]></code></pre>

<p>In C++, a File object is given to the constructor of the HashDBM class.</p>

<pre><code class="language-cpp"><![CDATA[HashDBM dbm(std::make_unique<PositionalParallelFile>());
dbm.Open("casket.tkh", true);
]]></code></pre>

<p>The File class is an interface class and you can implement subclasses of it to support any kind of storage which supports random access.</p>

<h3 id="tips_faq">Frequently Asked Questions</h3>

<dl>
<dt>Q: What does Tkrzw stand for?  How should I pronounce Tkrzw?</dt>
<dd>A: There's no meaning.  Please pronounce it as you like.</dd>
<dt>Q: What's the relationship between Tkrzw and Kyoto Cabinet (Tokyo Cabinet, QDBM)?</dt>
<dd>A: Tkrzw is a successor of them, utilizing C++17 features.  I believe Tkrzw is superior in many aspects to them.  Because API and data formats have been changed, there's no compatibility with the predecessors.</dd>
<dt>Q: Do you have a plan to write a database server like Kyoto Tycoon?</dt>
<dd>A: I don't so far.  I could consider it if there are demands.</dd>
<dt>Q: Do you have a plan to write a Win32 version?</dt>
<dd>A: I don't so far.  I don't have a Windows PC now.</dd>
</dl>

<h2 id="license">License</h2>

<p>Tkrzw is written maiinly by Mikio Hirabayashi, copyrighted by Google LLC, and distributed under the Apache license 2.0.  See the COPYING file in the package for detail.</p>

<p>Tkrzw is NOT an official Google product but developed by an indivisual as a personal hobby product.  It is copyrighted by the company due to his employment contract.  To contact the author, send an e-mail to &lt;hirarin@gmail.com&gt;.</p>

</article>
</body>
</html>
